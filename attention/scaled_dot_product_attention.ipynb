{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eef767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomer\n"
     ]
    }
   ],
   "source": [
    "with open ('../GPT/data/wikitext-103/wiki.train.tokens', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9104e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[793, 314, 142393, 131854, 109152, 18857, 314, 56319, 8675, 73, 16191, 860, 142393, 131854, 220, 18, 712, 464, 4060, 29, 109152, 350, 18938, 712, 15882, 99, 23868, 3385, 93460, 69686, 7052, 18368, 19913, 74066, 18, 1366, 11980, 887, 142393, 131854, 328, 290, 147762, 220, 18, 1546, 1366, 22378, 22653, 316, 472, 142393, 131854, 109152, 18857, 7539, 10198, 1366, 382, 261, 84570, 5430, 759, 12, 31, 8252, 3823, 2813, 9742, 656, 165189, 326, 10260, 5970, 3625, 395, 290, 9103, 23191, 61768, 887, 93105, 306, 8685, 220, 667, 16, 306, 10198, 1366, 480, 382, 290, 6914, 2813, 306, 290, 142393, 131854, 5594]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae0132",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product \n",
    "takes in a query key and value\n",
    "- Query : current word similarities are being calculated on\n",
    "- Key : all tokens in the sequence which will be computed with the query to provide similarities ( attention scores)\n",
    "- Value : the actual token \n",
    "\n",
    "## Method\n",
    "- Query and Key are dot product multiplied (matrix multiplication)\n",
    "- the value is then scaled \n",
    "- Masked if in decoder\n",
    "- softmax to calulate probs (attention weights)\n",
    "- matmul of attention weights with value\n",
    "\n",
    "```mermaid\n",
    "graph TD \n",
    "Q --> MM1[MatMul] \n",
    "K(K) --> MM1 \n",
    "MM1 --> Scale[Scale] \n",
    "Scale --> Mask[Mask opt.]\n",
    "Mask --> SoftMax[SoftMax]\n",
    "SoftMax --> MM2[MatMul]\n",
    "V ------> MM2\n",
    "MM2 --> Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a990cc4",
   "metadata": {},
   "source": [
    "# Multi-Head Attention \n",
    "\n",
    "Scaling the K,Q,V to multiple scaled Dot-Product Attention heads using linear projection.\n",
    "\n",
    "Once all of the values have been computed then concatenating them.  \n",
    "\n",
    "(optional addition of dropout or other reg techniques)\n",
    "\n",
    "linearly project them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f9ca329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_tokens = torch.tensor(tokens[:50000], dtype=torch.long, device='mps')\n",
    "torch.set_printoptions(linewidth=140)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8536513e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   793,    314, 142393, 131854, 109152,  18857,    314,  56319,   8675,     73], device='mps:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f756802",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 32 # number of tokens in each sequence\n",
    "BATCH_SIZE = 4 # number of parallel processes in a head\n",
    "\n",
    "def get_batch(tokens):\n",
    "    ix = torch.randint(len(tokens) - BLOCK_SIZE, (BATCH_SIZE,)) # random starting index for each sequence\n",
    "    try :\n",
    "        x = torch.stack([tokens[i:i+BLOCK_SIZE] for i in ix]) # stack the sequences into a\n",
    "        #batch makes it a 2d tensor in this case matrix which is batchsize x block_size\n",
    "        y = torch.stack([tokens[i+1:i+BLOCK_SIZE+1] for i in ix]) #shifted by one token\n",
    "        return x, y\n",
    "    except :\n",
    "        seq_len = tokens.shape[0] if isinstance(tokens, torch.Tensor) else len(tokens)\n",
    "        pad_token = 199999\n",
    "        if seq_len < BLOCK_SIZE:\n",
    "            pad_length = BLOCK_SIZE - seq_len\n",
    "            # Pad x and y with pad_token\n",
    "            x = torch.nn.functional.pad(x, (0, pad_length), value=pad_token)\n",
    "            y = torch.nn.functional.pad(y, (0, pad_length), value=pad_token)\n",
    "        print(f\"Sequence length: {seq_len}\")\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d7c95ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199999"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eot_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "857b60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "current, target = get_batch(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00fc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current.shape # currently batch_szie x block_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e801172",
   "metadata": {},
   "source": [
    "# B -> Batch_size (parallel computations in a head)\n",
    "# T -> Block_size (sequence length)\n",
    "# C -> Number of Embeddings\n",
    "\n",
    "# C the embeddings\n",
    "Extracted by transforming the tokens to a look up vector of vector representations. \n",
    "This extracts semantic and syntatic information about the tokens.\n",
    "This is the initial input feature for the transformer.\n",
    "Usually between 128 and 1024 but depending on hardware, scale to as needed\n",
    "Takes 2 values, the vocabulary size (ie number of unique tokens) and the embedding dim (based on compuational and memory constraints larger is better though)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9a718d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3669580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "019b3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size:int, embedding_dim:int, n_heads:int, dropout:float=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            block_size (int): Number of tokens in each sequence.\n",
    "            embedding_dim (int): Dimension of the embedding space.\n",
    "            n_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert embedding_dim % n_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.head_dim = embedding_dim // n_heads # number of parallel processes is the number of\n",
    "        # embeddings floor div by number of heads ie each head will work on a subset of the embeddings\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.qkv_projection = nn.Linear(embedding_dim, 3*embedding_dim, device='mps') # has the projection for all three stacked has\n",
    "        self.output_projection = nn.Linear(embedding_dim, embedding_dim, device='mps') # has the projection for all three stacked has\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, use_causal_mask: bool = False):\n",
    "\n",
    "        batch_size, block_size, embedding_dim = x.shape # parallel processes within a head, sequence length, embedding dimension\n",
    "        print(f\"batch_size: {batch_size}, block_size: {block_size}, embedding_dim: {embedding_dim}\")\n",
    "\n",
    "        qkv = self.qkv_projection(x)  # (batch_size, block_size, 3 * embedding_dim)\n",
    "        query, key, value = qkv.split(self.embedding_dim, dim=2)  # (batch_size, block_size, embedding_dim) each\n",
    "        print(f\"query.shape: {query.shape}, key.shape: {key.shape}, value.shape: {value.shape}\")\n",
    "        print(f\"query: {query[:2]}, key: {key[:2]}, value: {value[:2]}\")\n",
    "\n",
    "        # reshape to (batch_size, block_size, n_heads, embedding_dim // n_heads) this allows parallel processing as the tensor is\n",
    "        # split into its n_heads\n",
    "        query = query.view(batch_size, block_size, self.n_heads,self.head_dim).transpose(1, 2) # (batch_size, n_heads, block_size, embedding_dim // n_heads)\n",
    "        key = key.view(batch_size, block_size, self.n_heads,self.head_dim).transpose(1, 2) # (batch_size, n_heads, block_size, embedding_dim // n_heads)\n",
    "        value = value.view(batch_size, block_size, self.n_heads,self.head_dim).transpose(1, 2) # (batch_size, n_heads, block_size, embedding_dim // n_heads)\n",
    "        print(f\"query.shape: {query.shape}, key.shape: {key.shape}, value.shape: {value.shape}\")\n",
    "        print(f\"query: {query[:2]}, key: {key[:2]}, value: {value[:2]}\")\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2,-1))\n",
    "        print(f'attentions{attention_scores[:2]}')\n",
    "        scaled_attention_scores = attention_scores / math.sqrt(self.head_dim) # scaling\n",
    "        print(f'scaled_attention_scores{scaled_attention_scores[:2]}')\n",
    "\n",
    "        if use_causal_mask:\n",
    "            tril = torch.tril(torch.ones(block_size, block_size, device=scaled_attention_scores.device)).view(1, 1, block_size, block_size)\n",
    "            print(f'tril{tril[:2]}')\n",
    "            scaled_attention_scores = scaled_attention_scores.masked_fill(tril == 0, float('-inf'))\n",
    "            print(f'scaled_attention_scores{scaled_attention_scores[:2]}')\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_scores, dim=-1)\n",
    "        print(f'attention_weights{attention_weights[:2]}')\n",
    "        attention_weights = self.dropout_layer(attention_weights)\n",
    "        print(f'attention_weights{attention_weights[:2]}')\n",
    "\n",
    "        context_vectors = torch.matmul(attention_weights, value) # (batch_size, n_heads, block_size, embedding_dim // n_heads)\n",
    "        print(f'context_vectors{context_vectors[:2]}')\n",
    "        context_vectors = context_vectors.transpose(1, 2).contiguous().view(batch_size, block_size,embedding_dim) # (batch_size, block_size, embedding_dim)\n",
    "        print(f'context_vectors{context_vectors[:2]}')\n",
    "\n",
    "        output = self.output_projection(context_vectors) # (batch_size, block_size, embedding_dim)\n",
    "        print(f'output{output[:2]}')\n",
    "        output = self.dropout_layer(output)\n",
    "        print(f'output{output[0][0][0]}')\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "27500c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2df4f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(block_size=BLOCK_SIZE, embedding_dim=EMBEDDING_DIM, n_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "32202ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 128])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mha.forward(x, use_causal_mask=True)\n",
    "embedding_layer = nn.Embedding(tokenizer.n_vocab, EMBEDDING_DIM, device='mps')\n",
    "x_embedded = embedding_layer(x)\n",
    "x_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1972a021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 4, block_size: 32, embedding_dim: 128\n",
      "query.shape: torch.Size([4, 32, 128]), key.shape: torch.Size([4, 32, 128]), value.shape: torch.Size([4, 32, 128])\n",
      "query: tensor([[[ 0.0734, -0.5131, -1.5445,  ..., -0.1679,  0.5132, -0.2483],\n",
      "         [-0.2512,  0.3565,  0.3393,  ...,  0.0423, -0.3175,  0.3175],\n",
      "         [-0.0335, -0.6852,  0.7163,  ...,  0.0236,  0.3435, -0.0236],\n",
      "         ...,\n",
      "         [-0.7947,  0.1875, -0.5283,  ..., -0.5621, -0.2579, -0.1302],\n",
      "         [-0.7005,  0.2542, -0.4663,  ..., -0.2459, -0.2931,  0.4226],\n",
      "         [ 0.2155,  0.9124,  0.4845,  ...,  0.2412, -0.6053, -0.2347]],\n",
      "\n",
      "        [[-0.1661, -0.4890, -0.5011,  ...,  0.1103, -0.2260, -0.5821],\n",
      "         [-0.4166, -0.7103,  0.6181,  ..., -0.3068, -0.1211, -0.1450],\n",
      "         [-0.2102, -0.3381, -0.3500,  ...,  0.5178,  0.3451,  1.0514],\n",
      "         ...,\n",
      "         [ 0.0913, -0.9749, -0.6948,  ...,  0.6463,  0.4499, -0.6304],\n",
      "         [-0.7916, -0.8350,  0.5759,  ...,  0.3318,  0.4120,  1.6871],\n",
      "         [ 0.7143,  0.6597, -0.2820,  ..., -0.5112,  0.2259,  0.1803]]], device='mps:0', grad_fn=<SliceBackward0>), key: tensor([[[-0.1105,  0.0522, -0.1871,  ..., -0.7776,  0.1410,  0.7190],\n",
      "         [ 0.3941,  0.2477,  0.5586,  ..., -0.7657, -0.4031, -0.0583],\n",
      "         [ 0.6673,  0.2730,  0.7200,  ..., -0.3516,  0.2510,  0.4329],\n",
      "         ...,\n",
      "         [ 0.0595,  0.1755, -0.3849,  ..., -0.3656,  0.0645, -0.3106],\n",
      "         [-0.1125, -0.7274, -0.7008,  ...,  0.9076, -1.0051, -0.4923],\n",
      "         [-0.9875,  0.2860, -1.1387,  ..., -0.5431, -0.1849, -0.0239]],\n",
      "\n",
      "        [[ 0.5315, -0.3888, -0.4688,  ...,  0.8192, -0.1205,  0.3363],\n",
      "         [ 0.5747,  0.4619, -0.1488,  ...,  1.1342, -0.4346,  0.2941],\n",
      "         [-0.2876,  0.3684, -0.2922,  ..., -0.3087,  1.6083,  0.6274],\n",
      "         ...,\n",
      "         [ 0.8266, -0.8511,  0.0053,  ..., -0.4071,  0.1085,  0.0281],\n",
      "         [ 0.2380, -0.5165, -0.4850,  ...,  0.9985, -0.8446,  1.0393],\n",
      "         [ 0.2871, -0.2999, -0.2400,  ...,  0.6847, -1.7525,  1.1434]]], device='mps:0', grad_fn=<SliceBackward0>), value: tensor([[[ 0.1633, -0.2979, -0.4613,  ...,  0.3968, -0.8951,  0.2093],\n",
      "         [ 0.0746,  0.6923,  0.0780,  ..., -0.2759,  0.6880, -1.3274],\n",
      "         [ 0.2109,  0.2175, -0.3404,  ...,  0.4596,  1.1245,  0.2023],\n",
      "         ...,\n",
      "         [ 0.3316,  0.7235, -0.5042,  ...,  0.6628, -0.1871,  1.1552],\n",
      "         [ 0.6549,  0.6849, -0.8090,  ...,  1.0636,  0.1755, -0.8187],\n",
      "         [ 0.7770, -0.7136,  0.9397,  ..., -0.4013,  0.1065,  0.5490]],\n",
      "\n",
      "        [[ 0.1373,  0.2361, -0.0866,  ..., -0.2140,  0.5507, -0.2772],\n",
      "         [ 0.4732,  0.2747, -0.3891,  ...,  0.9892, -0.0796,  0.3201],\n",
      "         [ 0.1789, -0.0277,  0.0642,  ..., -0.1157, -0.3642, -0.2950],\n",
      "         ...,\n",
      "         [ 0.4696, -1.2179, -0.3676,  ..., -1.4815,  0.6490,  0.2732],\n",
      "         [-0.4861, -0.5452,  0.1931,  ...,  0.3564,  0.0154,  0.7958],\n",
      "         [-0.1435, -0.7489, -1.3167,  ...,  0.4454, -0.1153,  0.1222]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "query.shape: torch.Size([4, 4, 32, 32]), key.shape: torch.Size([4, 4, 32, 32]), value.shape: torch.Size([4, 4, 32, 32])\n",
      "query: tensor([[[[ 0.0734, -0.5131, -1.5445,  ..., -0.5448,  0.7693, -1.0842],\n",
      "          [-0.2512,  0.3565,  0.3393,  ...,  0.1397, -0.6345, -0.4134],\n",
      "          [-0.0335, -0.6852,  0.7163,  ..., -0.2934, -0.0896, -0.7586],\n",
      "          ...,\n",
      "          [-0.7947,  0.1875, -0.5283,  ..., -0.5558, -0.2341,  1.3286],\n",
      "          [-0.7005,  0.2542, -0.4663,  ..., -0.1564, -0.3085, -0.5354],\n",
      "          [ 0.2155,  0.9124,  0.4845,  ...,  0.2813, -0.9669, -0.2053]],\n",
      "\n",
      "         [[-0.8240, -0.2457,  0.0132,  ..., -0.3206, -0.5875,  0.0556],\n",
      "          [ 0.2744,  0.0527, -0.6792,  ...,  0.0475,  0.3580,  0.4973],\n",
      "          [ 0.1724,  0.0283, -0.1960,  ..., -0.5194,  1.1447,  0.3187],\n",
      "          ...,\n",
      "          [-0.3271,  0.0262,  0.6545,  ...,  0.2765, -0.2638,  0.1409],\n",
      "          [ 0.0799,  0.1405,  0.5990,  ..., -0.2183,  0.0835, -0.7333],\n",
      "          [-0.6073, -0.2446,  0.0068,  ...,  0.4240, -0.2970, -0.1938]],\n",
      "\n",
      "         [[-0.3608,  0.3593, -0.0415,  ...,  0.0402,  0.2082,  0.9392],\n",
      "          [ 0.5374, -0.0781, -1.1132,  ...,  0.2333, -0.0312, -0.0059],\n",
      "          [ 0.0653,  0.6378, -1.2989,  ..., -0.2574,  0.1507, -0.6170],\n",
      "          ...,\n",
      "          [ 0.9414, -0.3589,  0.0469,  ..., -0.1632, -0.8541,  0.3627],\n",
      "          [-0.0865, -0.3511, -0.5291,  ..., -0.2528,  0.1189,  0.4331],\n",
      "          [ 0.9988, -0.1346, -1.2972,  ..., -0.4005, -0.5067,  0.0191]],\n",
      "\n",
      "         [[-0.5813, -0.1656, -0.5189,  ..., -0.1679,  0.5132, -0.2483],\n",
      "          [ 0.3492,  0.6484, -0.4985,  ...,  0.0423, -0.3175,  0.3175],\n",
      "          [ 0.9473,  0.0113, -0.8386,  ...,  0.0236,  0.3435, -0.0236],\n",
      "          ...,\n",
      "          [-0.7859,  0.5247,  0.1948,  ..., -0.5621, -0.2579, -0.1302],\n",
      "          [ 0.1326, -0.2992,  1.4507,  ..., -0.2459, -0.2931,  0.4226],\n",
      "          [-0.5426,  0.2481,  0.3674,  ...,  0.2412, -0.6053, -0.2347]]],\n",
      "\n",
      "\n",
      "        [[[-0.1661, -0.4890, -0.5011,  ..., -1.0292,  0.3585,  0.5988],\n",
      "          [-0.4166, -0.7103,  0.6181,  ..., -1.0084, -0.6573,  0.0122],\n",
      "          [-0.2102, -0.3381, -0.3500,  ..., -0.4256, -0.0659, -0.0497],\n",
      "          ...,\n",
      "          [ 0.0913, -0.9749, -0.6948,  ...,  1.2134,  0.0483,  0.1477],\n",
      "          [-0.7916, -0.8350,  0.5759,  ..., -0.3513, -0.2251,  0.1517],\n",
      "          [ 0.7143,  0.6597, -0.2820,  ..., -0.8780, -0.8544,  0.6471]],\n",
      "\n",
      "         [[-0.6718, -0.1080,  1.2447,  ...,  0.3859,  0.2520, -0.5988],\n",
      "          [-0.2800,  0.1511,  1.2246,  ...,  0.2583, -0.7178, -0.9056],\n",
      "          [ 0.2472,  0.1968,  1.0215,  ...,  0.3961,  0.9027,  0.2701],\n",
      "          ...,\n",
      "          [ 0.0954, -0.4208, -0.0675,  ..., -0.9576, -0.6706,  0.3939],\n",
      "          [-0.3496, -1.0681, -0.3048,  ...,  0.1412, -0.2609, -0.8172],\n",
      "          [-0.7729, -1.1418, -0.6272,  ..., -1.2335, -0.3140,  0.2110]],\n",
      "\n",
      "         [[ 0.8547, -0.6691,  0.4394,  ...,  0.3259,  0.3166, -0.6833],\n",
      "          [-0.6651, -0.0072, -0.3676,  ..., -0.5344, -0.2481,  0.8071],\n",
      "          [-0.1733, -0.8345,  0.7226,  ...,  1.3516,  0.9626, -1.0838],\n",
      "          ...,\n",
      "          [ 0.1588, -0.2087, -0.0856,  ...,  1.7489, -0.5405,  0.1521],\n",
      "          [ 0.5621, -0.2022, -0.6504,  ...,  0.2165, -0.9026,  0.6075],\n",
      "          [ 0.1594,  0.8903, -0.1621,  ...,  0.6600, -0.6549,  0.2876]],\n",
      "\n",
      "         [[-0.6346,  0.8525, -0.1814,  ...,  0.1103, -0.2260, -0.5821],\n",
      "          [-0.3916, -0.1059,  0.0036,  ..., -0.3068, -0.1211, -0.1450],\n",
      "          [ 0.7458,  0.8184,  0.6429,  ...,  0.5178,  0.3451,  1.0514],\n",
      "          ...,\n",
      "          [-0.1473,  0.2511,  0.7176,  ...,  0.6463,  0.4499, -0.6304],\n",
      "          [ 0.2626, -0.1966, -0.1673,  ...,  0.3318,  0.4120,  1.6871],\n",
      "          [ 0.2767, -1.1763, -0.0079,  ..., -0.5112,  0.2259,  0.1803]]]], device='mps:0', grad_fn=<SliceBackward0>), key: tensor([[[[-0.1105,  0.0522, -0.1871,  ..., -1.0141, -1.4587, -0.2613],\n",
      "          [ 0.3941,  0.2477,  0.5586,  ..., -1.2257,  0.3396, -0.7892],\n",
      "          [ 0.6673,  0.2730,  0.7200,  ...,  0.0205,  0.9122,  0.0575],\n",
      "          ...,\n",
      "          [ 0.0595,  0.1755, -0.3849,  ...,  0.7350, -0.2316, -0.4310],\n",
      "          [-0.1125, -0.7274, -0.7008,  ...,  0.1785,  0.4966,  0.2293],\n",
      "          [-0.9875,  0.2860, -1.1387,  ...,  0.5805,  0.0414, -0.2166]],\n",
      "\n",
      "         [[ 0.8320,  0.1228, -1.0529,  ..., -0.5031, -0.0813,  0.1115],\n",
      "          [ 0.4392,  0.2976, -0.1673,  ..., -0.2915, -0.3281, -0.0628],\n",
      "          [-0.0861, -0.3861, -0.2117,  ..., -0.2213, -0.5048, -0.1091],\n",
      "          ...,\n",
      "          [ 0.1390,  0.3420,  0.5447,  ..., -0.4922, -0.6377,  0.6012],\n",
      "          [-0.5432,  0.9571, -0.2309,  ..., -0.7018,  0.9413, -0.1002],\n",
      "          [-2.1164,  0.7295,  0.1723,  ..., -0.4358, -0.3905, -0.0038]],\n",
      "\n",
      "         [[-0.1823,  0.3844,  0.4991,  ..., -0.0090,  1.1714, -0.5375],\n",
      "          [-0.2533, -0.5299, -0.4490,  ..., -0.2589,  0.1232, -0.8021],\n",
      "          [ 0.7479,  0.8582,  0.1053,  ..., -0.6283,  1.4039, -1.4808],\n",
      "          ...,\n",
      "          [-0.3234,  0.0743, -0.2257,  ...,  0.5041, -0.3296, -0.9489],\n",
      "          [-0.5185,  0.3338,  0.2196,  ..., -0.3550, -0.2088,  0.3032],\n",
      "          [ 0.1483, -0.1107, -0.9307,  ...,  0.2866,  0.3017, -0.1077]],\n",
      "\n",
      "         [[-0.2435,  1.2421,  0.6685,  ..., -0.7776,  0.1410,  0.7190],\n",
      "          [-0.5409, -0.0684,  0.3309,  ..., -0.7657, -0.4031, -0.0583],\n",
      "          [ 0.8887, -0.5320,  0.3630,  ..., -0.3516,  0.2510,  0.4329],\n",
      "          ...,\n",
      "          [-0.1818,  0.5995,  0.0510,  ..., -0.3656,  0.0645, -0.3106],\n",
      "          [-1.2610, -0.9623,  0.2307,  ...,  0.9076, -1.0051, -0.4923],\n",
      "          [ 0.4132, -1.1955, -0.1193,  ..., -0.5431, -0.1849, -0.0239]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5315, -0.3888, -0.4688,  ...,  0.0325,  1.0730,  0.1785],\n",
      "          [ 0.5747,  0.4619, -0.1488,  ..., -0.2344, -0.0545,  0.4134],\n",
      "          [-0.2876,  0.3684, -0.2922,  ..., -0.0189,  0.1223,  1.1325],\n",
      "          ...,\n",
      "          [ 0.8266, -0.8511,  0.0053,  ..., -0.3371,  0.5504, -0.1848],\n",
      "          [ 0.2380, -0.5165, -0.4850,  ..., -0.4987, -0.5120,  0.1516],\n",
      "          [ 0.2871, -0.2999, -0.2400,  ..., -0.0829,  0.3475,  0.5573]],\n",
      "\n",
      "         [[-1.0786, -0.0347,  0.9146,  ...,  0.5251, -0.3927, -0.1641],\n",
      "          [-0.5290,  0.1495, -0.0166,  ...,  0.5649, -0.2340,  0.7799],\n",
      "          [ 0.0671, -0.8553, -0.1655,  ...,  0.6513, -0.5846,  0.0473],\n",
      "          ...,\n",
      "          [ 0.1020, -0.4111,  0.1765,  ...,  0.0161, -0.5138, -0.1370],\n",
      "          [-0.6247, -0.1836, -0.5423,  ..., -0.5617, -0.7289,  0.3023],\n",
      "          [ 0.3058,  0.7693,  0.7960,  ..., -0.2119, -0.8102,  0.0410]],\n",
      "\n",
      "         [[-0.0912,  1.3585, -0.5687,  ...,  0.1967,  0.2705, -0.9393],\n",
      "          [-0.0762, -0.0365,  0.3609,  ...,  0.1594, -0.0293,  0.0156],\n",
      "          [-0.4125, -1.0826, -0.5305,  ..., -0.1434,  0.6710, -0.8353],\n",
      "          ...,\n",
      "          [-0.3934, -0.2283, -0.0562,  ...,  1.7346,  0.5404, -0.5621],\n",
      "          [ 0.5183,  0.4700, -0.2639,  ..., -0.3505, -0.2941,  0.1909],\n",
      "          [ 0.4275,  0.1222, -0.4794,  ...,  0.2250, -0.3655,  0.1240]],\n",
      "\n",
      "         [[ 0.1387, -1.0959, -0.0499,  ...,  0.8192, -0.1205,  0.3363],\n",
      "          [ 0.2733,  0.2373,  0.1880,  ...,  1.1342, -0.4346,  0.2941],\n",
      "          [ 0.6653,  0.4707, -0.1256,  ..., -0.3087,  1.6083,  0.6274],\n",
      "          ...,\n",
      "          [-0.5121, -0.3714,  1.2580,  ..., -0.4071,  0.1085,  0.0281],\n",
      "          [-0.7079,  0.6585, -0.9479,  ...,  0.9985, -0.8446,  1.0393],\n",
      "          [ 0.3861, -0.4330,  0.1290,  ...,  0.6847, -1.7525,  1.1434]]]], device='mps:0', grad_fn=<SliceBackward0>), value: tensor([[[[ 0.1633, -0.2979, -0.4613,  ...,  0.8944, -0.2362, -0.1084],\n",
      "          [ 0.0746,  0.6923,  0.0780,  ...,  1.0402,  0.7115, -0.4473],\n",
      "          [ 0.2109,  0.2175, -0.3404,  ..., -0.2729, -0.0022,  0.1331],\n",
      "          ...,\n",
      "          [ 0.3316,  0.7235, -0.5042,  ..., -0.4743, -0.0320, -1.0227],\n",
      "          [ 0.6549,  0.6849, -0.8090,  ...,  0.1730, -0.1769, -0.7135],\n",
      "          [ 0.7770, -0.7136,  0.9397,  ...,  0.4492, -0.1856, -0.4777]],\n",
      "\n",
      "         [[ 0.3790, -0.1209, -0.6842,  ...,  0.0228, -0.9195,  1.1546],\n",
      "          [ 0.2344,  0.4945, -1.4167,  ...,  0.2366, -0.2883,  0.2563],\n",
      "          [-0.3508, -0.2784,  0.2218,  ..., -1.0344,  0.9989, -0.2454],\n",
      "          ...,\n",
      "          [-0.0250,  0.3155, -0.1610,  ..., -1.1970, -0.4167, -0.2838],\n",
      "          [-0.0311, -0.0514,  0.6941,  ...,  0.8383, -0.1478, -1.0404],\n",
      "          [-0.4699, -0.7649, -0.4955,  ...,  0.5822,  0.3653, -0.8630]],\n",
      "\n",
      "         [[ 0.1639, -0.7088, -0.6833,  ..., -0.0965, -0.4955,  0.1024],\n",
      "          [ 0.4473, -0.1496, -0.0813,  ..., -0.2987,  0.0199, -0.8418],\n",
      "          [ 0.5720,  0.4202,  0.0363,  ..., -0.0104, -0.3823,  0.0856],\n",
      "          ...,\n",
      "          [ 0.0234,  0.3016,  0.0368,  ...,  0.2630, -0.0291,  0.2052],\n",
      "          [ 0.3181,  0.8499, -0.4453,  ...,  0.0366,  0.7187,  0.2994],\n",
      "          [ 0.6916,  0.0949,  0.3508,  ..., -0.4806, -1.1040, -0.0579]],\n",
      "\n",
      "         [[-0.2524, -0.3710, -0.3931,  ...,  0.3968, -0.8951,  0.2093],\n",
      "          [-0.0135,  0.3199,  0.3464,  ..., -0.2759,  0.6880, -1.3274],\n",
      "          [-0.6950, -0.7156, -0.4177,  ...,  0.4596,  1.1245,  0.2023],\n",
      "          ...,\n",
      "          [ 0.1994, -0.5050, -0.2383,  ...,  0.6628, -0.1871,  1.1552],\n",
      "          [-0.1639,  0.5774,  0.3808,  ...,  1.0636,  0.1755, -0.8187],\n",
      "          [ 0.3376,  0.7894,  0.3676,  ..., -0.4013,  0.1065,  0.5490]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1373,  0.2361, -0.0866,  ...,  0.7698,  0.2039, -0.2512],\n",
      "          [ 0.4732,  0.2747, -0.3891,  ...,  0.1662,  0.3216, -0.7982],\n",
      "          [ 0.1789, -0.0277,  0.0642,  ...,  0.0200,  0.1472, -0.2008],\n",
      "          ...,\n",
      "          [ 0.4696, -1.2179, -0.3676,  ...,  0.6706,  0.6681, -0.1099],\n",
      "          [-0.4861, -0.5452,  0.1931,  ..., -0.4125,  0.5016, -0.5160],\n",
      "          [-0.1435, -0.7489, -1.3167,  ...,  0.3620,  0.3187, -1.5291]],\n",
      "\n",
      "         [[-0.0637, -0.3181, -0.5030,  ..., -0.5099, -0.6624, -0.3140],\n",
      "          [-0.4561, -0.5372,  0.8250,  ..., -1.3748,  0.3876,  0.9409],\n",
      "          [-0.0821, -0.9055, -0.9033,  ..., -0.7186,  1.3003, -0.8494],\n",
      "          ...,\n",
      "          [ 0.0399, -0.2746,  1.0845,  ..., -0.8331,  1.6140,  0.1849],\n",
      "          [-0.5880, -0.4297,  0.3143,  ...,  0.3866,  0.8535, -0.5381],\n",
      "          [ 0.9883, -0.3673, -0.1586,  ...,  0.1487,  0.2215,  0.6499]],\n",
      "\n",
      "         [[-0.2250,  0.1447, -0.3243,  ...,  0.2230, -0.0489,  0.3835],\n",
      "          [-0.1438,  0.3352, -0.2718,  ...,  0.1639, -0.8925, -0.4075],\n",
      "          [-0.3692, -0.0020,  0.1693,  ..., -0.4568, -1.1198, -0.3503],\n",
      "          ...,\n",
      "          [-0.7766,  0.3833,  0.4157,  ...,  1.0292, -0.5143, -0.1277],\n",
      "          [-0.1527, -0.2251,  0.3602,  ...,  0.1818,  0.3454,  0.0918],\n",
      "          [-0.0076,  0.2646,  0.3985,  ..., -0.3916,  0.1701,  0.2838]],\n",
      "\n",
      "         [[-0.1091,  0.1469,  0.0099,  ..., -0.2140,  0.5507, -0.2772],\n",
      "          [ 0.2070, -0.1638,  0.1762,  ...,  0.9892, -0.0796,  0.3201],\n",
      "          [-0.4521,  0.2056,  0.0098,  ..., -0.1157, -0.3642, -0.2950],\n",
      "          ...,\n",
      "          [-0.8462, -1.0889,  0.8595,  ..., -1.4815,  0.6490,  0.2732],\n",
      "          [-0.2811, -0.8247, -0.3293,  ...,  0.3564,  0.0154,  0.7958],\n",
      "          [ 1.0749, -0.3099,  0.0211,  ...,  0.4454, -0.1153,  0.1222]]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "attentionstensor([[[[ 3.3796,  3.7103, -0.4480,  ..., -1.0773,  1.0553,  1.5692],\n",
      "          [ 0.2909, -2.5375,  2.3972,  ..., -0.3802, -1.1476,  1.3295],\n",
      "          [ 0.2334, -2.5962,  1.0121,  ...,  0.3910, -0.2966, -0.2567],\n",
      "          ...,\n",
      "          [-0.2955,  0.3357, -1.9213,  ..., -3.5506,  2.4337,  2.2630],\n",
      "          [-0.7109, -0.0091, -0.0419,  ...,  0.2195,  0.8005,  3.8133],\n",
      "          [ 1.9262,  3.2235, -0.2572,  ...,  2.3892, -0.1280, -2.1250]],\n",
      "\n",
      "         [[-1.3778,  0.4833, -0.4823,  ..., -1.7744, -0.9613,  1.6376],\n",
      "          [-2.6298,  1.1264,  2.0463,  ...,  1.4456,  2.5510, -2.5037],\n",
      "          [ 0.2186, -1.2878,  0.2659,  ..., -0.9349,  1.2141, -0.2443],\n",
      "          ...,\n",
      "          [ 0.1292, -2.9097, -2.0818,  ...,  0.9033,  0.6837,  3.7218],\n",
      "          [-0.2224,  1.0978, -1.8274,  ..., -0.8254, -4.1700, -1.7509],\n",
      "          [-2.4743,  0.8286,  0.8868,  ..., -3.2669, -0.8878,  0.1612]],\n",
      "\n",
      "         [[ 2.1370, -1.1564,  2.4762,  ..., -1.0457, -0.4351,  3.2486],\n",
      "          [-0.8689,  0.7993, -0.1357,  ..., -4.0220, -0.1825,  0.4965],\n",
      "          [-1.8768,  0.9294,  4.2635,  ...,  0.0197,  0.2887, -3.2890],\n",
      "          ...,\n",
      "          [-1.5082,  0.7933,  1.4821,  ..., -0.4157, -0.7500,  2.1044],\n",
      "          [-1.3174,  2.4243,  0.8748,  ...,  0.3226, -1.3591, -0.3156],\n",
      "          [-0.0470, -0.0546,  1.6254,  ...,  2.5319, -0.1768, -1.7704]],\n",
      "\n",
      "         [[ 1.5663,  2.9708, -2.4644,  ...,  2.7694,  3.2239,  0.2682],\n",
      "          [ 2.3280, -2.0810,  0.0092,  ...,  1.6079, -3.0039,  0.7636],\n",
      "          [ 0.8470, -2.4211,  1.0731,  ..., -0.0737, -2.9211,  4.1911],\n",
      "          ...,\n",
      "          [ 2.1783, -1.0344, -0.3837,  ..., -0.7643,  2.5239,  1.1148],\n",
      "          [-0.4186, -0.4103,  0.0765,  ...,  3.7343, -4.0294, -1.5390],\n",
      "          [-3.2373,  3.8810,  2.0508,  ..., -0.7279,  0.2618,  0.0591]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7588,  2.4145,  1.1903,  ...,  1.0711,  2.0755, -0.2492],\n",
      "          [-1.0198, -0.7358, -1.3671,  ...,  1.0776, -1.0649, -0.4794],\n",
      "          [-1.2212,  0.9677,  0.1386,  ...,  1.2119, -1.6610,  1.8820],\n",
      "          ...,\n",
      "          [ 0.2915,  3.4689, -2.8372,  ..., -0.0965, -0.5633,  2.4411],\n",
      "          [ 1.6890, -1.2326, -1.4523,  ..., -0.5486, -1.3123,  2.8704],\n",
      "          [-1.2796, -1.6035,  3.3218,  ..., -1.5419,  2.3133,  0.8962]],\n",
      "\n",
      "         [[ 2.2398, -3.5782, -1.2152,  ..., -2.3033, -0.7921, -1.7343],\n",
      "          [ 2.2508,  0.8577,  0.5158,  ...,  0.2689,  2.4409,  1.5146],\n",
      "          [ 0.6022, -0.0827,  0.2414,  ..., -1.2955, -0.7209, -0.5552],\n",
      "          ...,\n",
      "          [ 0.2425, -3.9657, -2.7572,  ...,  0.2792,  2.8086,  4.7917],\n",
      "          [ 0.7926, -0.1787,  2.1431,  ...,  0.8220, -0.2776, -1.3616],\n",
      "          [ 0.5612, -2.8414, -0.6592,  ...,  0.9374,  2.3766, -1.5593]],\n",
      "\n",
      "         [[-1.5070, -0.8625,  1.5044,  ...,  1.0117, -0.2223,  4.4851],\n",
      "          [-2.3412,  0.0336, -0.8275,  ..., -2.5542, -1.4615,  3.4254],\n",
      "          [ 0.5512,  0.6464,  1.1357,  ...,  3.5894, -4.0195,  0.1521],\n",
      "          ...,\n",
      "          [-2.5740,  2.2985, -1.6999,  ...,  1.7955, -3.1332, -0.2817],\n",
      "          [-0.7349, -2.0331, -2.6257,  ..., -1.6412, -0.7027, -2.4606],\n",
      "          [-1.2526, -0.3672, -4.1912,  ..., -1.4659, -1.8788, -1.3335]],\n",
      "\n",
      "         [[-1.5336, -0.6880, -0.0100,  ..., -0.5608,  4.0726, -1.4411],\n",
      "          [ 0.0100, -2.3160, -1.8019,  ...,  2.0646, -0.6814, -0.1566],\n",
      "          [-1.8580, -2.8663,  1.1477,  ...,  0.5607,  0.0224,  0.1160],\n",
      "          ...,\n",
      "          [ 1.9903,  1.0629, -0.0340,  ..., -0.4793, -1.0727,  0.9798],\n",
      "          [-0.0570, -0.1228,  1.0831,  ..., -0.2081, -1.4590,  1.6194],\n",
      "          [ 0.1841, -1.1704, -1.8008,  ...,  2.1699,  0.0220,  0.4488]]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "scaled_attention_scorestensor([[[[ 0.5974,  0.6559, -0.0792,  ..., -0.1904,  0.1866,  0.2774],\n",
      "          [ 0.0514, -0.4486,  0.4238,  ..., -0.0672, -0.2029,  0.2350],\n",
      "          [ 0.0413, -0.4589,  0.1789,  ...,  0.0691, -0.0524, -0.0454],\n",
      "          ...,\n",
      "          [-0.0522,  0.0593, -0.3396,  ..., -0.6277,  0.4302,  0.4000],\n",
      "          [-0.1257, -0.0016, -0.0074,  ...,  0.0388,  0.1415,  0.6741],\n",
      "          [ 0.3405,  0.5698, -0.0455,  ...,  0.4223, -0.0226, -0.3757]],\n",
      "\n",
      "         [[-0.2436,  0.0854, -0.0853,  ..., -0.3137, -0.1699,  0.2895],\n",
      "          [-0.4649,  0.1991,  0.3617,  ...,  0.2555,  0.4510, -0.4426],\n",
      "          [ 0.0386, -0.2277,  0.0470,  ..., -0.1653,  0.2146, -0.0432],\n",
      "          ...,\n",
      "          [ 0.0228, -0.5144, -0.3680,  ...,  0.1597,  0.1209,  0.6579],\n",
      "          [-0.0393,  0.1941, -0.3230,  ..., -0.1459, -0.7372, -0.3095],\n",
      "          [-0.4374,  0.1465,  0.1568,  ..., -0.5775, -0.1570,  0.0285]],\n",
      "\n",
      "         [[ 0.3778, -0.2044,  0.4377,  ..., -0.1849, -0.0769,  0.5743],\n",
      "          [-0.1536,  0.1413, -0.0240,  ..., -0.7110, -0.0323,  0.0878],\n",
      "          [-0.3318,  0.1643,  0.7537,  ...,  0.0035,  0.0510, -0.5814],\n",
      "          ...,\n",
      "          [-0.2666,  0.1402,  0.2620,  ..., -0.0735, -0.1326,  0.3720],\n",
      "          [-0.2329,  0.4286,  0.1546,  ...,  0.0570, -0.2403, -0.0558],\n",
      "          [-0.0083, -0.0097,  0.2873,  ...,  0.4476, -0.0313, -0.3130]],\n",
      "\n",
      "         [[ 0.2769,  0.5252, -0.4356,  ...,  0.4896,  0.5699,  0.0474],\n",
      "          [ 0.4115, -0.3679,  0.0016,  ...,  0.2842, -0.5310,  0.1350],\n",
      "          [ 0.1497, -0.4280,  0.1897,  ..., -0.0130, -0.5164,  0.7409],\n",
      "          ...,\n",
      "          [ 0.3851, -0.1829, -0.0678,  ..., -0.1351,  0.4462,  0.1971],\n",
      "          [-0.0740, -0.0725,  0.0135,  ...,  0.6601, -0.7123, -0.2721],\n",
      "          [-0.5723,  0.6861,  0.3625,  ..., -0.1287,  0.0463,  0.0105]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1341,  0.4268,  0.2104,  ...,  0.1893,  0.3669, -0.0441],\n",
      "          [-0.1803, -0.1301, -0.2417,  ...,  0.1905, -0.1882, -0.0847],\n",
      "          [-0.2159,  0.1711,  0.0245,  ...,  0.2142, -0.2936,  0.3327],\n",
      "          ...,\n",
      "          [ 0.0515,  0.6132, -0.5016,  ..., -0.0171, -0.0996,  0.4315],\n",
      "          [ 0.2986, -0.2179, -0.2567,  ..., -0.0970, -0.2320,  0.5074],\n",
      "          [-0.2262, -0.2835,  0.5872,  ..., -0.2726,  0.4089,  0.1584]],\n",
      "\n",
      "         [[ 0.3959, -0.6325, -0.2148,  ..., -0.4072, -0.1400, -0.3066],\n",
      "          [ 0.3979,  0.1516,  0.0912,  ...,  0.0475,  0.4315,  0.2677],\n",
      "          [ 0.1065, -0.0146,  0.0427,  ..., -0.2290, -0.1274, -0.0981],\n",
      "          ...,\n",
      "          [ 0.0429, -0.7010, -0.4874,  ...,  0.0493,  0.4965,  0.8471],\n",
      "          [ 0.1401, -0.0316,  0.3789,  ...,  0.1453, -0.0491, -0.2407],\n",
      "          [ 0.0992, -0.5023, -0.1165,  ...,  0.1657,  0.4201, -0.2757]],\n",
      "\n",
      "         [[-0.2664, -0.1525,  0.2659,  ...,  0.1789, -0.0393,  0.7929],\n",
      "          [-0.4139,  0.0059, -0.1463,  ..., -0.4515, -0.2584,  0.6055],\n",
      "          [ 0.0974,  0.1143,  0.2008,  ...,  0.6345, -0.7106,  0.0269],\n",
      "          ...,\n",
      "          [-0.4550,  0.4063, -0.3005,  ...,  0.3174, -0.5539, -0.0498],\n",
      "          [-0.1299, -0.3594, -0.4642,  ..., -0.2901, -0.1242, -0.4350],\n",
      "          [-0.2214, -0.0649, -0.7409,  ..., -0.2591, -0.3321, -0.2357]],\n",
      "\n",
      "         [[-0.2711, -0.1216, -0.0018,  ..., -0.0991,  0.7199, -0.2548],\n",
      "          [ 0.0018, -0.4094, -0.3185,  ...,  0.3650, -0.1204, -0.0277],\n",
      "          [-0.3285, -0.5067,  0.2029,  ...,  0.0991,  0.0040,  0.0205],\n",
      "          ...,\n",
      "          [ 0.3518,  0.1879, -0.0060,  ..., -0.0847, -0.1896,  0.1732],\n",
      "          [-0.0101, -0.0217,  0.1915,  ..., -0.0368, -0.2579,  0.2863],\n",
      "          [ 0.0325, -0.2069, -0.3183,  ...,  0.3836,  0.0039,  0.0793]]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "triltensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]], device='mps:0')\n",
      "scaled_attention_scorestensor([[[[ 0.5974,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.0514, -0.4486,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.0413, -0.4589,  0.1789,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.0522,  0.0593, -0.3396,  ..., -0.6277,    -inf,    -inf],\n",
      "          [-0.1257, -0.0016, -0.0074,  ...,  0.0388,  0.1415,    -inf],\n",
      "          [ 0.3405,  0.5698, -0.0455,  ...,  0.4223, -0.0226, -0.3757]],\n",
      "\n",
      "         [[-0.2436,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.4649,  0.1991,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.0386, -0.2277,  0.0470,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.0228, -0.5144, -0.3680,  ...,  0.1597,    -inf,    -inf],\n",
      "          [-0.0393,  0.1941, -0.3230,  ..., -0.1459, -0.7372,    -inf],\n",
      "          [-0.4374,  0.1465,  0.1568,  ..., -0.5775, -0.1570,  0.0285]],\n",
      "\n",
      "         [[ 0.3778,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1536,  0.1413,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3318,  0.1643,  0.7537,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.2666,  0.1402,  0.2620,  ..., -0.0735,    -inf,    -inf],\n",
      "          [-0.2329,  0.4286,  0.1546,  ...,  0.0570, -0.2403,    -inf],\n",
      "          [-0.0083, -0.0097,  0.2873,  ...,  0.4476, -0.0313, -0.3130]],\n",
      "\n",
      "         [[ 0.2769,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.4115, -0.3679,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.1497, -0.4280,  0.1897,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.3851, -0.1829, -0.0678,  ..., -0.1351,    -inf,    -inf],\n",
      "          [-0.0740, -0.0725,  0.0135,  ...,  0.6601, -0.7123,    -inf],\n",
      "          [-0.5723,  0.6861,  0.3625,  ..., -0.1287,  0.0463,  0.0105]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1341,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.1803, -0.1301,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.2159,  0.1711,  0.0245,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.0515,  0.6132, -0.5016,  ..., -0.0171,    -inf,    -inf],\n",
      "          [ 0.2986, -0.2179, -0.2567,  ..., -0.0970, -0.2320,    -inf],\n",
      "          [-0.2262, -0.2835,  0.5872,  ..., -0.2726,  0.4089,  0.1584]],\n",
      "\n",
      "         [[ 0.3959,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.3979,  0.1516,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.1065, -0.0146,  0.0427,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.0429, -0.7010, -0.4874,  ...,  0.0493,    -inf,    -inf],\n",
      "          [ 0.1401, -0.0316,  0.3789,  ...,  0.1453, -0.0491,    -inf],\n",
      "          [ 0.0992, -0.5023, -0.1165,  ...,  0.1657,  0.4201, -0.2757]],\n",
      "\n",
      "         [[-0.2664,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.4139,  0.0059,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.0974,  0.1143,  0.2008,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [-0.4550,  0.4063, -0.3005,  ...,  0.3174,    -inf,    -inf],\n",
      "          [-0.1299, -0.3594, -0.4642,  ..., -0.2901, -0.1242,    -inf],\n",
      "          [-0.2214, -0.0649, -0.7409,  ..., -0.2591, -0.3321, -0.2357]],\n",
      "\n",
      "         [[-0.2711,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [ 0.0018, -0.4094,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "          [-0.3285, -0.5067,  0.2029,  ...,    -inf,    -inf,    -inf],\n",
      "          ...,\n",
      "          [ 0.3518,  0.1879, -0.0060,  ..., -0.0847,    -inf,    -inf],\n",
      "          [-0.0101, -0.0217,  0.1915,  ..., -0.0368, -0.2579,    -inf],\n",
      "          [ 0.0325, -0.2069, -0.3183,  ...,  0.3836,  0.0039,  0.0793]]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "attention_weightstensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.6225, 0.3775, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3631, 0.2202, 0.4167,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0294, 0.0329, 0.0221,  ..., 0.0166, 0.0000, 0.0000],\n",
      "          [0.0282, 0.0319, 0.0317,  ..., 0.0332, 0.0368, 0.0000],\n",
      "          [0.0381, 0.0479, 0.0259,  ..., 0.0413, 0.0265, 0.0186]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3398, 0.6602, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3604, 0.2761, 0.3634,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0349, 0.0204, 0.0236,  ..., 0.0400, 0.0000, 0.0000],\n",
      "          [0.0328, 0.0414, 0.0247,  ..., 0.0295, 0.0163, 0.0000],\n",
      "          [0.0214, 0.0384, 0.0388,  ..., 0.0186, 0.0284, 0.0341]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4268, 0.5732, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1785, 0.2931, 0.5284,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0227, 0.0341, 0.0385,  ..., 0.0275, 0.0000, 0.0000],\n",
      "          [0.0259, 0.0501, 0.0381,  ..., 0.0346, 0.0257, 0.0000],\n",
      "          [0.0279, 0.0278, 0.0374,  ..., 0.0440, 0.0272, 0.0205]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.6856, 0.3144, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3843, 0.2157, 0.4000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0477, 0.0271, 0.0304,  ..., 0.0284, 0.0000, 0.0000],\n",
      "          [0.0261, 0.0262, 0.0285,  ..., 0.0544, 0.0138, 0.0000],\n",
      "          [0.0174, 0.0614, 0.0444,  ..., 0.0272, 0.0324, 0.0312]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4875, 0.5125, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2671, 0.3933, 0.3397,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0287, 0.0504, 0.0165,  ..., 0.0268, 0.0000, 0.0000],\n",
      "          [0.0399, 0.0238, 0.0229,  ..., 0.0268, 0.0235, 0.0000],\n",
      "          [0.0260, 0.0246, 0.0587,  ..., 0.0249, 0.0491, 0.0382]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5613, 0.4387, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3541, 0.3137, 0.3322,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0336, 0.0160, 0.0198,  ..., 0.0339, 0.0000, 0.0000],\n",
      "          [0.0339, 0.0285, 0.0430,  ..., 0.0341, 0.0280, 0.0000],\n",
      "          [0.0300, 0.0164, 0.0242,  ..., 0.0321, 0.0414, 0.0206]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3966, 0.6034, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3199, 0.3253, 0.3547,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0208, 0.0491, 0.0242,  ..., 0.0450, 0.0000, 0.0000],\n",
      "          [0.0319, 0.0254, 0.0228,  ..., 0.0272, 0.0321, 0.0000],\n",
      "          [0.0263, 0.0308, 0.0157,  ..., 0.0254, 0.0236, 0.0260]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.6014, 0.3986, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2827, 0.2365, 0.4808,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0470, 0.0399, 0.0329,  ..., 0.0304, 0.0000, 0.0000],\n",
      "          [0.0289, 0.0286, 0.0354,  ..., 0.0282, 0.0226, 0.0000],\n",
      "          [0.0293, 0.0230, 0.0206,  ..., 0.0416, 0.0284, 0.0307]]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "attention_weightstensor([[[[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.6916, 0.4195, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4035, 0.2447, 0.4630,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0366, 0.0245,  ..., 0.0184, 0.0000, 0.0000],\n",
      "          [0.0313, 0.0355, 0.0353,  ..., 0.0369, 0.0409, 0.0000],\n",
      "          [0.0423, 0.0532, 0.0000,  ..., 0.0459, 0.0294, 0.0000]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.7335, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4005, 0.3068, 0.4038,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0388, 0.0227, 0.0262,  ..., 0.0445, 0.0000, 0.0000],\n",
      "          [0.0364, 0.0460, 0.0274,  ..., 0.0328, 0.0181, 0.0000],\n",
      "          [0.0238, 0.0000, 0.0431,  ..., 0.0207, 0.0315, 0.0379]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4742, 0.6369, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.1983, 0.3257, 0.5871,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0252, 0.0379, 0.0428,  ..., 0.0306, 0.0000, 0.0000],\n",
      "          [0.0287, 0.0557, 0.0424,  ..., 0.0384, 0.0285, 0.0000],\n",
      "          [0.0310, 0.0309, 0.0416,  ..., 0.0488, 0.0303, 0.0000]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.7617, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4270, 0.2396, 0.4444,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0530, 0.0301, 0.0000,  ..., 0.0315, 0.0000, 0.0000],\n",
      "          [0.0290, 0.0291, 0.0317,  ..., 0.0605, 0.0153, 0.0000],\n",
      "          [0.0194, 0.0682, 0.0494,  ..., 0.0302, 0.0360, 0.0347]]],\n",
      "\n",
      "\n",
      "        [[[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.5416, 0.5695, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2968, 0.4370, 0.3774,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0319, 0.0560, 0.0184,  ..., 0.0298, 0.0000, 0.0000],\n",
      "          [0.0443, 0.0264, 0.0254,  ..., 0.0000, 0.0261, 0.0000],\n",
      "          [0.0289, 0.0273, 0.0653,  ..., 0.0276, 0.0546, 0.0425]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.6236, 0.4875, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3934, 0.3486, 0.3691,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0374, 0.0000, 0.0220,  ..., 0.0376, 0.0000, 0.0000],\n",
      "          [0.0376, 0.0317, 0.0478,  ..., 0.0378, 0.0312, 0.0000],\n",
      "          [0.0333, 0.0183, 0.0269,  ..., 0.0356, 0.0460, 0.0229]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4406, 0.6705, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3555, 0.3615, 0.3942,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0231, 0.0546, 0.0269,  ..., 0.0500, 0.0000, 0.0000],\n",
      "          [0.0354, 0.0282, 0.0254,  ..., 0.0302, 0.0356, 0.0000],\n",
      "          [0.0293, 0.0342, 0.0174,  ..., 0.0282, 0.0262, 0.0288]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.6682, 0.4429, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.2628, 0.5343,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0522, 0.0443, 0.0365,  ..., 0.0337, 0.0000, 0.0000],\n",
      "          [0.0322, 0.0318, 0.0393,  ..., 0.0313, 0.0251, 0.0000],\n",
      "          [0.0325, 0.0256, 0.0229,  ..., 0.0462, 0.0316, 0.0341]]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "context_vectorstensor([[[[ 0.1815, -0.3310, -0.5126,  ...,  0.9938, -0.2625, -0.1204],\n",
      "          [ 0.1442,  0.0844, -0.2863,  ...,  1.0549,  0.1351, -0.2626],\n",
      "          [ 0.1818,  0.1499, -0.3246,  ...,  0.4890,  0.0778, -0.0915],\n",
      "          ...,\n",
      "          [ 0.0580,  0.2108, -0.0163,  ...,  0.0145,  0.0895, -0.0689],\n",
      "          [ 0.1529,  0.1873, -0.0399,  ...,  0.0569,  0.0675, -0.1619],\n",
      "          [ 0.1341,  0.2619, -0.0054,  ...,  0.1146,  0.1029, -0.1124]],\n",
      "\n",
      "         [[ 0.4211, -0.1344, -0.7603,  ...,  0.0253, -1.0217,  1.2829],\n",
      "          [ 0.1720,  0.3627, -1.0392,  ...,  0.1735, -0.2115,  0.1880],\n",
      "          [ 0.0820, -0.0091, -0.6191,  ..., -0.3360, -0.0533,  0.4419],\n",
      "          ...,\n",
      "          [ 0.1695,  0.1773, -0.1914,  ..., -0.2061, -0.0241, -0.0023],\n",
      "          [ 0.0927,  0.1429, -0.2480,  ..., -0.0980, -0.0662, -0.0611],\n",
      "          [ 0.0731,  0.0647, -0.1806,  ..., -0.1489,  0.0913, -0.0655]],\n",
      "\n",
      "         [[ 0.1821, -0.7875, -0.7592,  ..., -0.1073, -0.5506,  0.1138],\n",
      "          [ 0.3626, -0.4314, -0.3758,  ..., -0.2360, -0.2223, -0.4876],\n",
      "          [ 0.5140,  0.0575, -0.1407,  ..., -0.1225, -0.3162, -0.2036],\n",
      "          ...,\n",
      "          [ 0.1067,  0.0851, -0.0318,  ..., -0.0413, -0.0925,  0.0981],\n",
      "          [-0.0777,  0.1267, -0.0395,  ..., -0.0649, -0.0574,  0.0748],\n",
      "          [ 0.0675,  0.0460, -0.1085,  ..., -0.0064, -0.1305,  0.0557]],\n",
      "\n",
      "         [[-0.2805, -0.4122, -0.4368,  ...,  0.4409, -0.9946,  0.2325],\n",
      "          [-0.1923, -0.2826, -0.2994,  ...,  0.3023, -0.6819,  0.1594],\n",
      "          [-0.4199, -0.3998, -0.2705,  ...,  0.3076,  0.2824, -0.1388],\n",
      "          ...,\n",
      "          [ 0.0293,  0.0777, -0.0985,  ...,  0.1308, -0.1361,  0.0314],\n",
      "          [ 0.0824,  0.0996, -0.1071,  ...,  0.0417, -0.1515,  0.0596],\n",
      "          [ 0.0614,  0.0448, -0.0417,  ...,  0.1301,  0.0259, -0.0066]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1525,  0.2623, -0.0962,  ...,  0.8553,  0.2265, -0.2791],\n",
      "          [ 0.3439,  0.2843, -0.2685,  ...,  0.5116,  0.2936, -0.5906],\n",
      "          [ 0.3150,  0.1796, -0.1715,  ...,  0.3086,  0.2566, -0.4991],\n",
      "          ...,\n",
      "          [ 0.2239,  0.0659, -0.1555,  ...,  0.0368,  0.1645, -0.0390],\n",
      "          [ 0.1325,  0.0815, -0.1397,  ..., -0.0847,  0.0369, -0.0169],\n",
      "          [ 0.1794,  0.0354, -0.0782,  ..., -0.0777,  0.1082, -0.1450]],\n",
      "\n",
      "         [[-0.0708, -0.3535, -0.5589,  ..., -0.5665, -0.7360, -0.3489],\n",
      "          [-0.2621, -0.4603,  0.0885,  ..., -0.9882, -0.2241,  0.2629],\n",
      "          [-0.2144, -0.6467, -0.2438,  ..., -0.9451,  0.3545, -0.1091],\n",
      "          ...,\n",
      "          [ 0.0107,  0.0245, -0.0092,  ..., -0.0640, -0.0146, -0.1172],\n",
      "          [-0.1738, -0.0386, -0.0249,  ..., -0.1061,  0.1163, -0.0468],\n",
      "          [-0.1194, -0.0111,  0.0301,  ..., -0.0588,  0.1285,  0.0079]],\n",
      "\n",
      "         [[-0.2500,  0.1608, -0.3604,  ...,  0.2478, -0.0543,  0.4261],\n",
      "          [-0.1955,  0.2885, -0.3251,  ...,  0.2081, -0.6199, -0.1042],\n",
      "          [-0.2775,  0.1718, -0.1468,  ..., -0.0415, -0.7814, -0.1491],\n",
      "          ...,\n",
      "          [-0.2123,  0.2024, -0.0447,  ..., -0.0086, -0.0492, -0.1682],\n",
      "          [-0.2020,  0.0820, -0.0592,  ...,  0.0316,  0.0190, -0.1186],\n",
      "          [-0.1153,  0.0961, -0.1628,  ..., -0.1353,  0.0934, -0.0327]],\n",
      "\n",
      "         [[-0.1212,  0.1632,  0.0110,  ..., -0.2378,  0.6119, -0.3080],\n",
      "          [ 0.0188,  0.0256,  0.0847,  ...,  0.2952,  0.3327, -0.0434],\n",
      "          [-0.1872,  0.0668,  0.0515,  ...,  0.1982, -0.2155, -0.0735],\n",
      "          ...,\n",
      "          [-0.1510,  0.0737,  0.0419,  ..., -0.0472, -0.1487, -0.1204],\n",
      "          [-0.0456,  0.0165,  0.0561,  ...,  0.0325, -0.1194, -0.0884],\n",
      "          [-0.1270,  0.0485,  0.1474,  ...,  0.2049, -0.0405,  0.0756]]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "context_vectorstensor([[[ 0.1815, -0.3310, -0.5126,  ...,  0.4409, -0.9946,  0.2325],\n",
      "         [ 0.1442,  0.0844, -0.2863,  ...,  0.3023, -0.6819,  0.1594],\n",
      "         [ 0.1818,  0.1499, -0.3246,  ...,  0.3076,  0.2824, -0.1388],\n",
      "         ...,\n",
      "         [ 0.0580,  0.2108, -0.0163,  ...,  0.1308, -0.1361,  0.0314],\n",
      "         [ 0.1529,  0.1873, -0.0399,  ...,  0.0417, -0.1515,  0.0596],\n",
      "         [ 0.1341,  0.2619, -0.0054,  ...,  0.1301,  0.0259, -0.0066]],\n",
      "\n",
      "        [[ 0.1525,  0.2623, -0.0962,  ..., -0.2378,  0.6119, -0.3080],\n",
      "         [ 0.3439,  0.2843, -0.2685,  ...,  0.2952,  0.3327, -0.0434],\n",
      "         [ 0.3150,  0.1796, -0.1715,  ...,  0.1982, -0.2155, -0.0735],\n",
      "         ...,\n",
      "         [ 0.2239,  0.0659, -0.1555,  ..., -0.0472, -0.1487, -0.1204],\n",
      "         [ 0.1325,  0.0815, -0.1397,  ...,  0.0325, -0.1194, -0.0884],\n",
      "         [ 0.1794,  0.0354, -0.0782,  ...,  0.2049, -0.0405,  0.0756]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "outputtensor([[[-1.3189e-01,  6.3322e-01,  6.9444e-02,  ...,  5.7207e-01, -4.0416e-01,  2.4997e-01],\n",
      "         [ 1.9579e-01,  2.3896e-01, -4.8609e-01,  ...,  4.4853e-01, -1.9055e-01,  2.7513e-01],\n",
      "         [ 8.8778e-02,  6.9286e-02, -1.1575e-01,  ..., -2.8310e-01, -3.0493e-01,  5.0843e-01],\n",
      "         ...,\n",
      "         [ 6.1368e-02,  7.6535e-02, -1.9239e-01,  ..., -5.6350e-02, -1.1018e-01, -5.0468e-02],\n",
      "         [ 2.0146e-02,  1.2332e-01, -2.1431e-01,  ..., -5.9249e-03, -1.0444e-01, -1.7719e-02],\n",
      "         [ 1.0534e-01,  1.5040e-01, -2.1437e-01,  ..., -1.0351e-01, -5.0814e-02, -1.3261e-01]],\n",
      "\n",
      "        [[-1.6106e-01,  2.6129e-01,  5.1522e-01,  ...,  3.1321e-02,  3.4644e-01, -2.1035e-02],\n",
      "         [-1.3098e-01, -1.9479e-01,  7.3886e-01,  ...,  3.5081e-02,  8.6081e-02,  2.4147e-01],\n",
      "         [-1.1181e-01, -1.6033e-01,  3.4862e-01,  ...,  6.3154e-02,  4.6092e-01,  7.1340e-02],\n",
      "         ...,\n",
      "         [ 3.5978e-02, -2.6506e-02,  2.1215e-02,  ..., -9.0225e-02, -6.6387e-02,  1.8791e-01],\n",
      "         [ 2.5374e-02,  4.9867e-02,  1.6614e-04,  ..., -5.4865e-02, -9.9430e-02,  1.1443e-01],\n",
      "         [ 2.3452e-02, -2.8454e-03,  1.6503e-02,  ..., -9.5772e-02, -1.4828e-01,  1.1061e-01]]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "outputtensor([[[-0.0000e+00,  7.0358e-01,  7.7160e-02,  ...,  6.3563e-01, -4.4907e-01,  2.7774e-01],\n",
      "         [ 2.1755e-01,  2.6551e-01, -5.4010e-01,  ...,  4.9836e-01, -2.1173e-01,  3.0570e-01],\n",
      "         [ 9.8642e-02,  7.6984e-02, -1.2861e-01,  ..., -3.1456e-01, -3.3882e-01,  5.6492e-01],\n",
      "         ...,\n",
      "         [ 6.8187e-02,  8.5039e-02, -2.1376e-01,  ..., -6.2611e-02, -1.2242e-01, -0.0000e+00],\n",
      "         [ 0.0000e+00,  1.3702e-01, -2.3812e-01,  ..., -0.0000e+00, -1.1605e-01, -1.9688e-02],\n",
      "         [ 0.0000e+00,  1.6711e-01, -2.3819e-01,  ..., -1.1501e-01, -5.6460e-02, -1.4734e-01]],\n",
      "\n",
      "        [[-1.7896e-01,  2.9032e-01,  5.7247e-01,  ...,  3.4801e-02,  3.8493e-01, -2.3372e-02],\n",
      "         [-1.4554e-01, -2.1643e-01,  8.2095e-01,  ...,  3.8979e-02,  9.5645e-02,  2.6830e-01],\n",
      "         [-1.2423e-01, -1.7814e-01,  3.8735e-01,  ...,  7.0171e-02,  5.1214e-01,  7.9267e-02],\n",
      "         ...,\n",
      "         [ 3.9976e-02, -2.9452e-02,  2.3572e-02,  ..., -1.0025e-01, -0.0000e+00,  0.0000e+00],\n",
      "         [ 2.8194e-02,  0.0000e+00,  1.8460e-04,  ..., -6.0962e-02, -1.1048e-01,  1.2714e-01],\n",
      "         [ 2.6058e-02, -3.1616e-03,  1.8336e-02,  ..., -1.0641e-01, -1.6476e-01,  1.2289e-01]]], device='mps:0', grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000e+00,  7.0358e-01,  7.7160e-02,  ...,  6.3563e-01, -4.4907e-01,  2.7774e-01],\n",
       "         [ 2.1755e-01,  2.6551e-01, -5.4010e-01,  ...,  4.9836e-01, -2.1173e-01,  3.0570e-01],\n",
       "         [ 9.8642e-02,  7.6984e-02, -1.2861e-01,  ..., -3.1456e-01, -3.3882e-01,  5.6492e-01],\n",
       "         ...,\n",
       "         [ 6.8187e-02,  8.5039e-02, -2.1376e-01,  ..., -6.2611e-02, -1.2242e-01, -0.0000e+00],\n",
       "         [ 0.0000e+00,  1.3702e-01, -2.3812e-01,  ..., -0.0000e+00, -1.1605e-01, -1.9688e-02],\n",
       "         [ 0.0000e+00,  1.6711e-01, -2.3819e-01,  ..., -1.1501e-01, -5.6460e-02, -1.4734e-01]],\n",
       "\n",
       "        [[-1.7896e-01,  2.9032e-01,  5.7247e-01,  ...,  3.4801e-02,  3.8493e-01, -2.3372e-02],\n",
       "         [-1.4554e-01, -2.1643e-01,  8.2095e-01,  ...,  3.8979e-02,  9.5645e-02,  2.6830e-01],\n",
       "         [-1.2423e-01, -1.7814e-01,  3.8735e-01,  ...,  7.0171e-02,  5.1214e-01,  7.9267e-02],\n",
       "         ...,\n",
       "         [ 3.9976e-02, -2.9452e-02,  2.3572e-02,  ..., -1.0025e-01, -0.0000e+00,  0.0000e+00],\n",
       "         [ 2.8194e-02,  0.0000e+00,  1.8460e-04,  ..., -6.0962e-02, -1.1048e-01,  1.2714e-01],\n",
       "         [ 2.6058e-02, -3.1616e-03,  1.8336e-02,  ..., -1.0641e-01, -1.6476e-01,  1.2289e-01]],\n",
       "\n",
       "        [[ 2.8187e-01,  1.7490e-01,  3.0180e-02,  ..., -0.0000e+00, -1.1923e-02,  5.6082e-01],\n",
       "         [-1.4946e-01,  3.2616e-01,  2.3435e-01,  ..., -1.4295e-01, -1.8789e-01,  2.6639e-01],\n",
       "         [ 2.2536e-01,  1.7673e-01,  1.4862e-01,  ...,  9.5143e-02, -3.2884e-01,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 1.6915e-01,  3.8005e-02, -8.3527e-02,  ..., -1.9125e-01, -0.0000e+00,  1.1802e-01],\n",
       "         [ 1.4964e-01,  6.8770e-02, -6.3953e-02,  ..., -2.1171e-01, -1.2445e-01,  6.1801e-02],\n",
       "         [ 1.7366e-01,  0.0000e+00, -1.4149e-01,  ..., -2.5493e-01, -2.3983e-01,  7.0365e-02]],\n",
       "\n",
       "        [[-7.0019e-01, -4.9788e-01,  5.3182e-01,  ...,  2.2105e-01, -2.2214e-01, -3.3031e-01],\n",
       "         [-6.0193e-01, -3.7112e-01,  5.8521e-01,  ...,  2.2698e-01, -2.8471e-01,  3.8207e-02],\n",
       "         [-5.1516e-01, -7.0945e-02,  2.5688e-01,  ...,  1.9674e-01, -1.9543e-02,  2.2048e-01],\n",
       "         ...,\n",
       "         [-2.1869e-02,  8.2401e-02,  3.8794e-02,  ...,  4.5846e-02, -3.7303e-02,  3.4532e-02],\n",
       "         [-1.2379e-02,  1.4510e-01, -8.8126e-03,  ..., -4.7120e-02, -9.2336e-02,  1.2707e-01],\n",
       "         [ 1.4261e-03,  0.0000e+00, -3.8981e-02,  ...,  6.9456e-02,  7.7660e-03,  1.5430e-01]]], device='mps:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha(x_embedded, use_causal_mask=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
