{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3146c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('data/wikitext-2-raw/wiki.train.raw', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open('data/wikitext-2-raw/wiki.valid.raw', 'r') as f:\n",
    "    valid = f.read()\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76269d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10918892"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d9640",
   "metadata": {},
   "source": [
    "getting vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e99a132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'デ', '膀', 'ה', '၁', 'л', '厂', 'ﷲ', '術', 'バ', '钱', '\\\\', 'ാ', '၈', 'Ⴃ', 'β', '琦', 'ွ', '泗', 'ლ', '陳', 'ს', '翠', '淹', '月', '4', 'ʲ', 'ქ', '運', '義', '里', '斯', 'ν', 'ค', 'Y', 'Ñ', 'ベ', 'ई', 'œ', 'á', 'ं', 'キ', 'ช', '라', 'Ś', '霹', 'ォ', '譜', '河', 'გ', '齋', '神', '≡', 'ਾ', '乃', 'ტ', 'Ü', 'K', 'i', '七', 'С', '化', '浮', 'ä', '灼', '%', '′', '錡', '揺', '円', 'μ', '園', '建', 'ħ', 'τ', '邱', 'Ⴈ', '雞', 'ゲ', '部', '砲', '毅', '似', 'l', '世', '行', 'ž', 'ჶ', 'ώ', '詩', '藕', '張', 'マ', 'ỹ', 'ι', '−', 'J', 'ź', 'χ', 'ァ', 'п', 'Х', '്', '剛', 'ל', 'd', '御', '杜', 'य', '广', 'イ', '제', 'ˈ', '天', 'ვ', '†', '道', 'ί', '夕', ':', '€', 'ʁ', 'ã', 'ɢ', 'К', 'ာ', 'リ', '致', '關', '野', '♀', 'す', 'j', '�', 'ę', 'ם', 'พ', 'ว', '良', '焼', '廬', '酈', 'ち', 'ấ', ';', '蝶', 'O', '解', '熱', 'ὁ', 'ᵻ', 'ك', '下', '無', '\\n', '微', '／', 'ო', '德', '/', '過', '―', 'ω', ',', 'ø', '転', 'й', 'ē', '宗', 'ệ', '完', 'დ', '\\x94', 'ə', '州', 'י', '}', 'ม', 'ב', '台', '樸', '套', 'コ', 'Æ', '.', 'Ž', 'か', 'ق', 'ょ', '연', '漢', '堤', 'т', 'Χ', 'ള', '洪', 'た', 'ン', '守', 'Α', '¡', 'ộ', 'ܐ', 'ჷ', 'စ', 'š', 'ჵ', '殿', '望', 'ჟ', 'X', 'ш', 'ഹ', 'N', 'ჹ', 'क', 'ἰ', '其', 'ま', 'ܢ', 'İ', 'ჳ', 'ဂ', 'Ł', 'ţ', '包', '千', 'प', 'Ω', '星', 'נ', 'Б', 'Þ', 'ნ', 'ိ', '譌', 'ỗ', 'ც', 'フ', '主', '和', '花', '부', 'ṅ', '8', 'ý', '劇', 'ɡ', '⅔', '战', 'Ē', '光', '°', 'Ş', '李', 'ܗ', '陈', 'န', '&', '\\u200b', 'א', '靂', 'ñ', '玉', 'れ', '村', 'Π', '6', 'ɐ', '`', 'Τ', '〈', '王', 'ノ', 'ḍ', 'ָ', 'ﻋ', 'ˌ', '‐', 'ィ', 'グ', 'ੱ', 'ɜ', '遇', '果', '春', 'ြ', '来', 'つ', 'ή', 'ჩ', '̯', 'Á', 'ੁ', '萬', '誓', 'ี', 'ά', 'А', 'ы', 'ج', 'ဇ', '紀', 'ア', '女', 'Ⴕ', 'ṣ', 'ר', '充', 'า', 'д', '隊', '畫', 'm', 'ミ', '政', '束', 'Я', 'บ', 'ه', 'ศ', 'û', '澄', 'ッ', '名', 'إ', 'ก', 'c', 'γ', 'ヴ', 'Š', '豪', 'ჲ', '制', '後', 'Ö', '林', 'さ', 'モ', 'น', 'ิ', 'Ⴂ', '2', 'ọ', 'আ', '曦', '芳', '寺', 'ц', 'с', 'ː', '?', 'Ⴀ', 'є', '♭', 'م', '明', '0', 'ჭ', '母', '遠', 'ʒ', '祈', '豫', 'स', 'ắ', '马', '²', '±', '機', '์', '銃', '‡', 'ゆ', 'ı', '顯', '‑', 'º', '籠', 'ă', '彩', 'ث', '谭', '防', 'တ', 'の', '彼', \"'\", '稽', 'ỏ', '乙', 'み', '征', '่', '府', 'ญ', '9', '中', '坂', '正', 'ჱ', 'x', '殻', 'ּ', 'ر', '̃', '場', 'ब', 'α', 'ř', '독', 'ェ', 'അ', '塘', 's', 'v', '助', 'Ș', '圣', '繹', '費', 'ת', '舍', 'Ō', 'り', 'ש', 'レ', 'ɾ', 'ễ', '法', '空', '書', '♯', 'ต', 'î', '若', 'H', '~', 'ყ', '蝴', 'Đ', 'ั', '澤', 'Ø', 'έ', 'ἀ', 'ז', 'ì', 'π', 'ț', 'ǎ', '岳', '₹', '牌', 'き', 'Ā', 'ي', 'ɑ', 'A', 'f', 'ह', 'ပ', 'い', 'Г', '憶', '彌', 'g', 'ば', '雲', '織', 'T', '鐵', 'Ó', '堂', 'ू', '・', 'ح', 'ʊ', 'р', 'σ', '出', 'ブ', '願', '錄', 'ฐ', 'お', '_', '人', 'オ', '唐', 'く', '⅓', '球', 'ရ', 'လ', '咲', ')', '者', 'ب', '市', '珂', 'プ', 'ó', 'θ', 'ù', '二', 'ム', '芥', 'س', '前', '妙', '阿', 'შ', '跡', '龍', '子', '•', 'ु', '橘', 'µ', 'パ', 'თ', 'ç', 'ג', 'أ', '̥', '£', 'ห', 'ü', '→', 'ʔ', '琪', '印', 'Ľ', '君', 'し', 'у', 'х', '體', 'ن', 'ण', '็', '影', '十', '<', '§', 'ā', 'đ', '|', 'ܲ', 'я', '>', '判', 'ე', 'Î', 'I', 'ό', 'ъ', 'â', '理', 'ʼ', '要', 'Ä', '金', 'と', 'Ν', '云', 'B', '集', 'ზ', '립', '都', 'κ', '朝', '☉', 'ء', 'ภ', 'ų', '古', '足', 'u', 'ů', '定', '誰', 'ō', 'ด', 'Å', '奈', 'ß', '္', '清', 'ュ', 'ャ', '’', '观', '赤', 'ỳ', 'ク', '動', 'ê', '所', 'ṯ', 'ὑ', '兵', 'ซ', '´', 'テ', 'В', '3', 'П', '̍', 'ল', 'ხ', 'ズ', '學', 'ș', '善', 'ś', '物', '☫', 'อ', 'y', 'Ż', 'ص', '½', '贵', 'м', 'ð', 'あ', 'ứ', 'ɽ', '在', '·', '楚', '霖', '扈', '饾', 'Z', 'ܠ', '英', '灯', 'ö', 'ძ', 'ハ', '—', 'チ', '(', '肖', '生', 'ֶ', '～', 'ɳ', 'ガ', '皇', 'V', '7', 'è', '絵', 'う', '約', 'ਅ', 'z', '\\ufeff', 'ń', '田', 'ş', '應', 'É', 'ふ', 'ბ', 'ô', '¿', 'و', '芽', '景', '波', 'P', 'ו', 'ớ', 'ʕ', '式', 'わ', '依', 'M', '劉', 'る', 'ò', 'မ', 'द', '汉', '́', '寶', '郭', 'ヘ', '昌', '瓊', 'ん', 'ฮ', 'ܕ', '华', '″', 'г', 'უ', '⁄', 'ż', 'S', 'č', 'ゃ', '…', '覺', '藥', 'ạ', '座', 'ა', 'პ', '泣', '記', '鍵', '背', 'η', 'ش', '裝', '玄', 'ぐ', 'У', 'F', '宋', 'Ç', '丙', 'は', '火', '拉', '„', '之', '駢', 'ル', 'ả', 'व', '耕', '懷', '³', 'ậ', 'ė', '沂', 'に', 'न', 'R', '畢', 'ไ', 'o', '詔', 'ف', '一', 'Ⴌ', 'ầ', '謎', 'ป', '少', 'ṭ', 'н', '裁', '珙', '同', '铁', 'r', 'ナ', '北', 'კ', 'å', '्', 'ʾ', '譚', '宝', 'ع', 'ფ', 'რ', '傳', '͍', '階', 'ũ', 'د', '信', 'ة', 'Ⴟ', 'が', 'к', 'ж', 'め', '$', '寝', '平', '技', '辛', '*', '大', 'ჺ', 'ჸ', '安', 'ʃ', 'ɔ', 'ֵ', '〉', 'Φ', 'เ', 'ܝ', 'ذ', 'ा', 'サ', 'ɪ', '小', '椎', 'ɒ', 'ダ', '雪', '心', 'া', '著', '放', '拳', 'Ú', 'ჯ', '四', 'ग', ' ', 'ï', '尾', 'ひ', 'q', 'ल', '鋼', '알', '‘', 'Č', '邦', 'म', 'Í', 'হ', '秋', 'ง', '¥', '刷', '狐', '棘', 'D', '真', '대', 'ψ', 'ȯ', '變', '蘄', '[', 'ח', 'け', 'ס', '湯', '¢', 'k', 'ɛ', 'ο', '₣', 'ḷ', '挑', 'ὀ', 'G', '疆', 'त', 'カ', 'b', 'ら', '灵', '臂', '桜', 'ṃ', '^', 'U', '\\x93', '本', 'ラ', 'ǔ', 'ḥ', '趙', '南', 'í', '日', '颜', 'ḏ', 'セ', '迪', 'ơ', '錦', '再', '\"', 'n', 'ぎ', '辨', 'ł', 'p', 'エ', '်', '玩', 'ן', '白', '夫', 'ジ', '¾', '჻', '規', '၂', 'ף', 'ご', 'ョ', 'ず', 'ะ', '竹', 'ą', 'æ', '+', 'ύ', '”', '宫', 'ု', '未', 'מ', '祠', 'в', 'ܬ', '梶', 'е', 'წ', 'を', 'E', '國', 'ド', 'ღ', 'ě', 'მ', '皮', 'þ', '侗', '板', 'ĕ', 'ë', 'ṛ', 'a', '1', 'ि', 'ữ', '儚', 'ل', '巳', 'خ', '九', 'ε', 'ニ', '陽', 'แ', 'ネ', '้', 'Ḥ', 'ゥ', 'а', 'र', '보', '্', '緬', '精', 'ษ', 'ğ', 'ス', '≤', '#', '作', '誡', '–', '水', 'ֹ', 'ぜ', 'ਹ', 'タ', 'צ', '၇', '去', '攻', 'အ', 'ṇ', 'だ', '韓', 'ɫ', 'こ', 'Δ', 'λ', 'な', '榮', '必', 'ρ', 'à', 't', 'פ', 'Q', '具', 'é', 'ร', '高', 'ึ', 'ロ', '方', 'з', '付', 'ჴ', 'ย', '選', 'ư', 'Ĕ', '興', ']', '①', 'ξ', '頒', '〜', 'ī', 'e', 'L', '姚', '思', 'ς', '逆', 'も', '立', '戦', 'и', 'ー', '類', '沙', 'W', 'ト', 'ь', '散', '言', '旦', '今', '庆', '胡', 'ا', 'अ', '全', 'ツ', '楊', '文', 'ú', '統', '蘇', '-', 'ほ', 'ਲ', 'ณ', '许', 'ส', 'ჰ', 'C', 'ć', 'ị', '狸', '表', '蘿', '六', 'े', '可', '!', '병', '孟', 'ʿ', 'ʋ', 'о', '@', 'ū', '“', '史', '忠', '番', '5', 'シ', 'δ', '憑', '吳', '驗', '晋', '病', 'っ', 'ै', 'υ', '師', '礮', 'ю', 'ი', '旭', '軒', '瘡', 'ت', 'Κ', '命', '川', '₤', '周', 'ܵ', '=', '３', '๊', 'ử', '箋', '×', 'ล', '⚳', '愛', 'Р', '拱', '山', 'w', 'ʻ', 'h'}\n",
      "\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz|}~¡¢£¥§°±²³´µ·º½¾¿ÁÄÅÆÇÉÍÎÑÓÖ×ØÚÜÞßàáâãäåæçèéêëìíîïðñòóôöøùúûüýþĀāăąćČčĐđĒēĔĕėęěğħīİıĽŁłńŌōœřŚśŞşŠšţũūůųźŻżŽžơưǎǔȘșțȯɐɑɒɔəɛɜɡɢɪɫɳɽɾʁʃʊʋʒʔʕʲʻʼʾʿˈˌː̥̯͍́̃̍ΑΔΚΝΠΤΦΧΩάέήίαβγδεηθικλμνξοπρςστυχψωόύώАБВГКПРСУХЯавгдежзийклмнопрстухцшъыьюяєֵֶָֹּאבגהוזחילםמןנסףפצרשתءأإابةتثجحخدذرسشصعفقكلمنهويܐܕܗܝܠܢܬܲܵंअईकगणतदनपबमयरलवसहािुूेै्আলহা্ਅਲਹਾੁੱഅളഹാ്กคงชซญฐณดตนบปพภมยรลวศษสหอฮะัาิีึเแไ็่้๊์ဂစဇတနပမရလအာို္်ြွ၁၂၇၈ႠႢႣႨႬႵႿაბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰჱჲჳჴჵჶჷჸჹჺ჻ᵻḍḏḤḥḷṃṅṇṛṣṭṯạảấầậắễệịọỏỗộớứửữỳỹἀἰὀὁὑ​‐‑–—―‘’“”„†‡•…′″⁄₣₤€₹⅓⅔→−≡≤①☉☫♀♭♯⚳〈〉〜あいうおかがきぎくぐけこごさしすずぜただちっつとなにのはばひふほまみめもゃゆょらりるれわをんァアィイゥェエォオカガキクグゲコサシジスズセタダチッツテデトドナニネノハバパフブプヘベマミムモャュョラリルレロンヴ・ー一七下世丙中主乃之乙九二云人今付似作侗依信傳儚充光全六兵其具円再出判制刷前剛劇劉助動包化北十千华南印厂去古可台史同名君吳周命和咲唐善四國園圣在坂堂堤場塘夕大天夫奈套女妙姚子孟學守安宋完宗定宝宫寝寶寺小少尾山岳川州巳市師平广庆府座廬建式張彌彩影彼征後御微德心必忠思愛憑憶應懷战戦所扈技拉拱拳挑揺攻放政散文斯方日旦旭昌明星春晋景曦書月望朝未本李村杜束来板林果桜梶棘椎楊楚榮樸橘機正殻殿毅母水汉沂沙河法泗波泣洪浮淹清湯漢澄澤火灯灵灼無焼熱牌物狐狸玄玉王玩珂珙球理琦琪瓊生田畢番畫疆病瘡白皇皮真砲礮祈神祠秋稽空立竹箋籠精紀約統絵緬織繹義翠者耕肖背胡膀臂致興舍良芥花芳芽若英萬著藕藥蘄蘇蘿蝴蝶行術表裁裝要規覺观解言記詔詩誓誡誰謎譌譚譜變许谭豪豫費贵赤趙足跡軒転辛辨迪逆遇運過道遠選邦邱部郭都酈里野金銃鋼錄錡錦鍵鐵钱铁關防阿陈陳陽隊階集雞雪雲霖霹靂韓頒願類顯颜饾駢驗马體高齋龍대독라립병보부알연제ﷲﻋ﻿／３～�\n",
      "1118\n"
     ]
    }
   ],
   "source": [
    "print(set(text+valid)) # set of all characters\n",
    "chars = sorted(list(set(text+valid))) #sorted and in a list\n",
    "vocab_size = len(chars) # number of unique characters\n",
    "print(''.join(chars)) # print all characters\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7382575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def filter_english_chars(vocab_string):\n",
    "    allowed_chars = set(string.ascii_letters + string.digits + string.punctuation + ' ' + '\\n') # Include space\n",
    "    filtered_vocab = ''.join(c for c in vocab_string if c in allowed_chars)\n",
    "    return filtered_vocab, allowed_chars\n",
    "\n",
    "vocab, chars = filter_english_chars(''.join(chars))\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8f783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text_by_vocab(text_var, allowed_chars):\n",
    "    \"\"\"Removes characters from a text string that are not in the allowed character set.\"\"\"\n",
    "    filtered_text = ''.join(c for c in text_var if c in allowed_chars)\n",
    "    return filtered_text\n",
    "\n",
    "text = filter_text_by_vocab(text, chars)\n",
    "valid = filter_text_by_vocab(valid, chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c724545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senj no Valkyria 3 : Unrecorded Chronicles ( Japanese : 3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers .\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e7813",
   "metadata": {},
   "source": [
    "# tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d69e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 65, 30, 30, 61, 87, 94, 61, 70, 30, 37]\n",
      "hello world\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title 'Very Simple char level tokeniser'\n",
    "s_to_i={ch: i for i, ch in enumerate(chars)} # map from character to index string to interger\n",
    "i_to_s={i: ch for i, ch in enumerate(chars)} # map from index to character\n",
    "\n",
    "encode = lambda s: [s_to_i[c] for c in s] # function to encode a string to a list of integers\n",
    "decode = lambda l: ''.join([i_to_s[i] for i in l]) # function to decode a list of integers to a string\n",
    "print(encode('hello world'))\n",
    "print(decode(encode('hello world')))\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9358f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using tiktoken\n",
    "# import tiktoken\n",
    "# encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "# print(encoding.encode(\"hello world\"))\n",
    "\n",
    "# print(encoding.encode(\"goodbye world\"))\n",
    "# print(encoding.decode(encoding.encode(\"hello world\")))\n",
    "# vocab_size = encoding.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3508c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken.list_encoding_names() x# list of all available encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abdec7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f955cff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "torch.set_printoptions(linewidth=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e57537b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([87, 50, 87, 89, 87, 28, 38, 30,  5, 15, 70, 21, 38, 87, 78, 95, 70, 61, 17, 21, 14, 30, 65, 44, 87, 93, 93, 93, 87, 89, 87, 50, 87,\n",
       "        50, 87, 46, 65, 17, 43, 87, 17, 61, 87, 28, 38, 30,  5, 15, 70, 21, 38, 87, 13, 87, 41, 87, 12, 17, 70, 65, 14, 61, 70, 37, 65, 37,\n",
       "        87, 78, 95, 70, 61, 17, 21, 14, 30, 65, 44, 87, 25, 87, 33, 38, 18, 38, 17, 65, 44, 65, 87, 41, 87, 13, 87, 57, 87, 30, 21, 54, 87,\n",
       "        60], device='mps:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long, device=device) # convert the text to a tensor of integers\n",
    "test = torch.tensor(encode(valid), dtype=torch.long, device=device) # convert the validation text to a tensor of integers\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b492a61",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ea237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: tensor([87], device='mps:0'), target: 50\n",
      "context: tensor([87, 50], device='mps:0'), target: 87\n",
      "context: tensor([87, 50, 87], device='mps:0'), target: 89\n",
      "context: tensor([87, 50, 87, 89], device='mps:0'), target: 87\n",
      "context: tensor([87, 50, 87, 89, 87], device='mps:0'), target: 28\n",
      "context: tensor([87, 50, 87, 89, 87, 28], device='mps:0'), target: 38\n",
      "context: tensor([87, 50, 87, 89, 87, 28, 38], device='mps:0'), target: 30\n",
      "context: tensor([87, 50, 87, 89, 87, 28, 38, 30], device='mps:0'), target: 5\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "x = data[:block_size] # first 256 characters\n",
    "y = data[1:block_size+1] # next 256 characters\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1] # context is the first character\n",
    "    target = y[i] # target is the next character\n",
    "    print(f'context: {context}, target: {target}') # print the context and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b28f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[87, 94, 38, 44, 87, 37, 65, 14],\n",
      "        [46, 87, 62, 70, 21, 95, 38, 17],\n",
      "        [95, 87, 30, 61, 14, 38, 30, 87],\n",
      "        [57, 87, 94, 38, 44, 87, 70, 65]], device='mps:0')\n",
      "tensor([[94, 38, 44, 87, 37, 65, 14, 21],\n",
      "        [87, 62, 70, 21, 95, 38, 17, 54],\n",
      "        [87, 30, 61, 14, 38, 30, 87, 18],\n",
      "        [87, 94, 38, 44, 87, 70, 65, 30]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # 4 parallel sequences\n",
    "block_size = 8 # 8 characters in each sequence\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    batch_data = data if split == 'train' else test\n",
    "    ix = torch.randint(len(batch_data) - block_size, (batch_size,)) # random starting index for each sequence\n",
    "    x = torch.stack([batch_data[i:i+block_size] for i in ix]) # stack the sequences into a batch makes it a 2d tensor in this case matrix which is batchsize x block_size\n",
    "    # x is a batch of sequences, each sequence is block_size characters long\n",
    "    # y is the same as x but shifted one character to the right\n",
    "    # so that the target is the next character in the sequence\n",
    "    y = torch.stack([batch_data[i+1:i+block_size+1] for i in ix]) # stack the targets into a batch\n",
    "\n",
    "    return x, y # return the inputs and targets\n",
    "\n",
    "x,y = get_batch('train') # get a batch of data\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643816c",
   "metadata": {},
   "source": [
    "# Baseline model Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79b11e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96])\n",
      "tensor(4.8024, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiGram(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # get the logits for the input indices\n",
    "        # logits is a batch_size x block_size x vocab_size tensor\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C) # reshape to match pytorchs crossentropy loss function\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # calculate the loss\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # generate new tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # last time step logits  since bigram  B,C\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #B,1\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # B, T+1\n",
    "        return idx\n",
    "\n",
    "model = BiGram(vocab_size)\n",
    "model.to(device)\n",
    "logits, loss = model(x,y) # get the output of the model\n",
    "print(logits.shape) # should be batch_size x block_size , vocab_size\n",
    "print(loss)\n",
    "# best loss is 1/vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c517e021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='mps:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 1), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "753c035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B9!--4WHIJ}+72/>M'~=\"8Qi1/8-ZZ*&4;~0HyEjF\n",
      "].0dNttt&`<HG!TFe5s|zIHC:9C\n",
      "[l{G:Egyzm:\\0[VRGQrKe5r1zZ'D&ezN5>25(%OB'lSq\\elDX=I}A&u^[<sQU(frhH1//-l$GO3@TB9Kz\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=150)[0].tolist())) # generate 100 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dac0b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3) # create an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5df7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200 # number of iterations to evaluate the model\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    # estimate the loss of the model on the given split\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # set the model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "598c698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss 5.1492, valid loss 5.1425\n",
      "iter 1000: train loss 3.8272, valid loss 3.8245\n",
      "iter 2000: train loss 3.0755, valid loss 3.0742\n",
      "iter 3000: train loss 2.7188, valid loss 2.7188\n",
      "iter 4000: train loss 2.5703, valid loss 2.5703\n",
      "iter 5000: train loss 2.5111, valid loss 2.5113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 8\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, valid loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# get a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(x, y)\n\u001b[0;32m---> 13\u001b[0m         losses[k] \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# set the model back to training mode\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "block_size = 256\n",
    "max_iters = 10000\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % 1000 == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"iter {iter}: train loss {losses['train']:.4f}, valid loss {losses['val']:.4f}\")\n",
    "\n",
    "    x, y = get_batch('train') # get a batch of data\n",
    "    logits, loss = model(x, y) # get the output of the model\n",
    "    optimizer.zero_grad(set_to_none=True) # zero the gradients\n",
    "    loss.backward() # backpropagate the loss\n",
    "    optimizer.step() # update the weights\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49156a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bect , idicin , eirblo poyede dydollint ierrusind ct a wege Monghat s me Insinglofit t n } angs , tic hind Gea]9 . \" cremetative% d#an he ntheninse Augin ws o+ @, . on coofans im Cadennet'smpoun(S be! 'satheskieee stacor cun mpe ten stosis prdem fos fund pinig 220 . tse ffiXI imlemay wofode ly ollas ator = athe a gh t sh \n",
      " hio , I-@ MMened wongemew ~Mouprerainininoriree an iusy , ana , ththamenouom\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=400)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8107eb00",
   "metadata": {},
   "source": [
    "# Math trick for self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77adc6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2\n",
    "\n",
    "x = torch.randn(B,T,C, device=device)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26f59947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5390,  0.1042],\n",
       "         [ 1.1619,  0.3173],\n",
       "         [ 1.1988,  0.2161],\n",
       "         [ 1.0397,  0.2219],\n",
       "         [ 0.9977,  0.2251],\n",
       "         [ 1.0850,  0.3045],\n",
       "         [ 0.8459,  0.2788],\n",
       "         [ 0.6131,  0.2094]],\n",
       "\n",
       "        [[ 1.8131,  1.9998],\n",
       "         [ 1.1748,  0.9645],\n",
       "         [ 0.7729,  1.0423],\n",
       "         [ 0.7829,  0.9763],\n",
       "         [ 0.6232,  1.1038],\n",
       "         [ 0.5313,  0.8700],\n",
       "         [ 0.5583,  0.8190],\n",
       "         [ 0.7612,  0.7031]],\n",
       "\n",
       "        [[ 0.3468,  0.4603],\n",
       "         [ 0.2709,  0.4539],\n",
       "         [-0.1692, -0.2836],\n",
       "         [ 0.1307, -0.3205],\n",
       "         [-0.1025, -0.2423],\n",
       "         [-0.1626, -0.3073],\n",
       "         [-0.2459, -0.1954],\n",
       "         [-0.2065,  0.0042]],\n",
       "\n",
       "        [[-1.1061,  0.8020],\n",
       "         [ 0.7374,  0.0813],\n",
       "         [ 0.2316,  0.0218],\n",
       "         [ 0.5822,  0.0797],\n",
       "         [ 0.1636, -0.2286],\n",
       "         [ 0.1084, -0.1601],\n",
       "         [ 0.0923,  0.0497],\n",
       "         [ 0.1941,  0.1916]]], device='mps:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = torch.zeros(B,T,C, device=device)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # get the previous tokens\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # calculate the mean of the previous tokens\n",
    "\n",
    "    #inefficirnt best to use matmul\n",
    "xbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d446245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7., 6.],\n",
      "        [4., 6., 5.],\n",
      "        [0., 4., 0.]])\n",
      "tensor([[ 6., 17., 11.],\n",
      "        [ 6., 17., 11.],\n",
      "        [ 6., 17., 11.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,3)).float()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "c= a @ b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b76d6f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7., 6.],\n",
      "        [4., 6., 5.],\n",
      "        [0., 4., 0.]])\n",
      "tensor([[ 2.,  7.,  6.],\n",
      "        [ 6., 13., 11.],\n",
      "        [ 6., 17., 11.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#even more efficient using a triangle mat\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0,10,(3,3)).float()\n",
    "print(a)\n",
    "print(b)\n",
    "c = a @ b\n",
    "print(c) # nb first couple of rows will be different but really only the last row matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d2bff",
   "metadata": {},
   "source": [
    "Vectorised to be even faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8168312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[[ 0.5390,  0.1042],\n",
      "         [ 1.1619,  0.3173],\n",
      "         [ 1.1988,  0.2161],\n",
      "         [ 1.0397,  0.2219],\n",
      "         [ 0.9977,  0.2251],\n",
      "         [ 1.0850,  0.3045],\n",
      "         [ 0.8459,  0.2788],\n",
      "         [ 0.6131,  0.2094]],\n",
      "\n",
      "        [[ 1.8131,  1.9998],\n",
      "         [ 1.1748,  0.9645],\n",
      "         [ 0.7729,  1.0423],\n",
      "         [ 0.7829,  0.9763],\n",
      "         [ 0.6232,  1.1038],\n",
      "         [ 0.5313,  0.8700],\n",
      "         [ 0.5583,  0.8190],\n",
      "         [ 0.7612,  0.7031]],\n",
      "\n",
      "        [[ 0.3468,  0.4603],\n",
      "         [ 0.2709,  0.4539],\n",
      "         [-0.1692, -0.2836],\n",
      "         [ 0.1307, -0.3205],\n",
      "         [-0.1025, -0.2423],\n",
      "         [-0.1626, -0.3073],\n",
      "         [-0.2459, -0.1954],\n",
      "         [-0.2065,  0.0042]],\n",
      "\n",
      "        [[-1.1061,  0.8020],\n",
      "         [ 0.7374,  0.0813],\n",
      "         [ 0.2316,  0.0218],\n",
      "         [ 0.5822,  0.0797],\n",
      "         [ 0.1636, -0.2286],\n",
      "         [ 0.1084, -0.1601],\n",
      "         [ 0.0923,  0.0497],\n",
      "         [ 0.1941,  0.1916]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T,T, device=device)) # create a lower triangular matrix\n",
    "weights = weights / weights.sum(1, keepdim=True) # normalize the weights\n",
    "xbow2 = weights @ x # matrix multiply the weights with the input\n",
    "# weights is a T x T matrix and x is a B x T x C matrix\n",
    "# torch will simulate a B for weights which will be one anyway\n",
    "# so the result will be a B x T x C matrix\n",
    "print(xbow2.shape)\n",
    "#xbow2 becomes identical to xbow at least in the final dim which is the one which matters\n",
    "print(xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "263be036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2) # check if the two tensors are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb40a36",
   "metadata": {},
   "source": [
    "More optims with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de26d1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T, device=device))\n",
    "weights = torch.zeros((T,T), device=device) # normalize the weights\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # set the weights to -inf where they are 0\n",
    "weights = F.softmax(weights, dim=-1) # apply softmax to the weights\n",
    "xbow3 = weights @ x # matrix multiply the weights with the input\n",
    "torch.allclose(xbow, xbow3) # check if the two tensors are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee169f8",
   "metadata": {},
   "source": [
    "# Head of Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d97a9fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "tensor([[-2.5207e-01,  1.0414e+00,  5.5443e-01, -7.3752e-01,  5.9558e-01, -2.0277e-01,  8.2716e-01,  1.8483e-01, -1.2360e-01,  3.4101e-01,\n",
      "          3.4797e-01,  3.3328e-01, -1.8975e-01, -4.4118e-01,  5.3352e-01,  7.1370e-01],\n",
      "        [-4.0713e-01,  2.8695e-01,  3.0739e-01, -8.8783e-01,  3.5704e-01, -3.1265e-02,  5.1177e-01,  2.4270e-01,  2.6314e-01,  2.5746e-01,\n",
      "          2.6357e-01,  2.7188e-01,  6.3774e-02, -4.5481e-01,  7.2480e-01,  3.2292e-01],\n",
      "        [-3.4338e-01,  1.3117e-01,  5.6705e-02, -4.1080e-01,  2.3881e-01,  8.4171e-02,  2.3994e-01,  7.4744e-02,  3.5817e-01,  6.9215e-02,\n",
      "          3.1321e-01,  4.0953e-01, -5.3430e-03, -1.5554e-01,  3.0700e-01,  3.6238e-01],\n",
      "        [-1.0911e-01,  3.5296e-01,  2.7960e-01, -4.4277e-01,  1.7990e-01, -2.1221e-02,  1.3903e-01,  1.2606e-01,  2.7081e-01,  1.7274e-01,\n",
      "          2.1523e-01,  3.9167e-01,  2.0649e-02, -6.0739e-02,  4.6006e-01,  2.2900e-01],\n",
      "        [-8.5048e-02,  1.7762e-01,  1.6764e-01, -3.2877e-01,  1.6215e-01, -6.5533e-02,  3.2003e-01,  2.6011e-01, -7.2409e-02,  2.9403e-01,\n",
      "          1.0228e-01,  2.5804e-01,  2.3147e-01, -9.4666e-02,  5.6039e-01, -9.2901e-03],\n",
      "        [ 1.6022e-01,  3.0714e-01,  4.5997e-01, -4.5448e-01, -1.4727e-01, -8.4189e-04,  2.1817e-01, -1.4863e-01, -1.4388e-01,  5.3407e-01,\n",
      "          9.0884e-02,  1.1183e-01,  3.2191e-02,  1.3226e-01,  5.9918e-01,  1.8837e-02],\n",
      "        [ 7.0913e-02,  1.9655e-01,  4.2527e-01, -2.4177e-01,  1.6492e-01, -1.0123e-01, -2.6314e-02, -2.1060e-01,  1.5820e-02,  2.8189e-01,\n",
      "          1.9132e-01,  1.9311e-01,  8.8929e-02,  8.3724e-02,  4.5823e-01,  1.6598e-01],\n",
      "        [ 1.1839e-01,  1.3932e-01,  3.0834e-01, -3.5427e-01, -5.4642e-02,  4.8204e-02,  1.7114e-01, -9.9138e-02, -1.9872e-01,  3.9410e-01,\n",
      "          1.4055e-01, -4.3759e-02,  9.3628e-02,  1.5397e-01,  5.5238e-01, -7.8636e-02]], device='mps:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "head_size=16\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C, device=device) # create a random tensor\n",
    "\n",
    "key = nn.Linear(C,head_size,bias=False, device=device)\n",
    "query = nn.Linear(C,head_size,bias=False, device=device)\n",
    "value = nn.Linear(C,head_size,bias=False, device=device)\n",
    "\n",
    "\n",
    "k = key(x) # B,T,head_size\n",
    "q = query(x) # B,T,head_size\n",
    "v = value(x)\n",
    "\n",
    "weights = q @ k.transpose(-2,-1) * head_size**-.5# B,T,T transpose the last two dimensions ,scaled attention\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T, device=device)) # create a lower triangular matrix\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) # set the weights to -inf where they are 0\n",
    "weights = F.softmax(weights, dim=-1) # apply softmax to the weights\n",
    "out = weights @ v\n",
    "print(out.shape) # should be B x T x head_size\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74670854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3842, 0.6158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3365, 0.3164, 0.3471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3316, 0.2071, 0.2283, 0.2330, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1711, 0.1746, 0.1925, 0.1806, 0.2812, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1120, 0.1663, 0.1434, 0.1832, 0.1432, 0.2519, 0.0000, 0.0000],\n",
      "        [0.1051, 0.1633, 0.1690, 0.1052, 0.0914, 0.1447, 0.2213, 0.0000],\n",
      "        [0.0961, 0.1089, 0.0925, 0.1179, 0.1610, 0.1800, 0.0965, 0.1471]], device='mps:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(weights[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
